---
title             : "Stability and Change in Adults' Literacy and Numeracy Skills: Evidence from two Large-scale Panel Studies"
shorttitle        : "Stability and Change in Adults' Literacy and Numeracy"

author: 
  - name          : "Clemens M. Lechner"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "B2, 1, 68159 Mannheim, Germany"
    email         : "clemens.lechner@gesis.org"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Funding acquisition
      - Supervision
      - Conceptualization
      - Data curation
      - Writing - Original Draft 
      - Writing - Review & Editing
      - Methodology 
  - name          : "Britta Gauly"
    affiliation   : "1"
    role:
      - Data curation
      - Methodology
      - Formal analysis
      - Writing - Review & Editing
  - name          : "Ai Miyamoto"
    affiliation   : "2"
    role: 
    - Writing - original draft
    - Writing - Review & Editing
  - name          : "Alexandra Wicht"
    affiliation   : "3"
    role:
      - Data curation
      - Formal analysis
      - Writing - Review & Editing
  
affiliation:
  - id            : "1"
    institution   : "GESIS - Leibniz Institute for the Social Sciences, Mannheim, Germany"
  - id            : "2"
    institution   : "University of Freiburg, Germany"
  - id            : "3"
    institution   : "University of Siegen, Germany"
  

authornote: |
  The authors would like to thank student assistants Marcel Kappes and Thomas Knopf for help during data curation and analysis.
  
abstract: |
 This study seeks to answer two questions about the development of literacy skills (reading competence) and numeracy skills (math competence) during adulthood: First, how stable are these skills in adulthood – and if they change, does change involve gains or losses? Second, how is change distributed in the population and in demographic subgroups? To answer these questions, we use data from two German large-scale surveys: The PIAAC longitudinal study (PIAAC-L; *N* = 1,775) and the National Educational Panel Study (NEPS; *N* = 3,087). Both surveys offer repeated, high-quality measures of adults’ literacy and numeracy spaced three (PIAAC-L) to six (NEPS) years apart. As complementary measures of change, we examine  Mean-level change and ($\Delta_{T_1,T_2}$) and rank-order consistencies ($r_{T_1,T_2}$) of skills in the total population as well as in major sociodemographic subgroups defined by age, gender, and education. We used plausible value (PV) methodology to account for measurement error in skills. Results revealed that literacy and numeracy are hightly but not perfectly stable over time. Whereas mean-level change was negligible for both skills and studies, there was considerable variation in change across individuals and subgroups. Few of these subgroup differences replicated across studies. We find some evidence for moderate skill gains during young adulthood (18–29 years) and losses in old age (55+ years), resembling age profiles from cross-sectional studies. Rank-order consistencies ranged from *r* = .83 for literacy and *r* = 0.81 for numeracy over three years in PIAAC_L to *r* = .65  for literacy and *r* = .75 for numeracy over six years in NEPS, with some variation across subgroups in NEPS but not PIAAC-L. Our findings suggest that adults' literacy and numeracy are not set in stone and can change even over relatively short time periods. They can serve as a benchmark against which to compare future longitudinal findings on stability and change in adult skills.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "literacy, numeracy, skills, intelligence, developoment, adulthood"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : yes
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---
<!-- APA 7 https://github.com/crsh/papaja/issues/342-->

```{r setup, include = FALSE}
# Load required packages

getwd()
library(papaja)
library(tidyverse)
library(glue)
library(sjlabelled)
library(mice)
library(miceadds)
library(mitools)
library(srvyr)

# BibLateX Reference file
r_refs("r-references.bib")

# Load raw data and results
map(c(
  "math_neps.Rda", "reading_neps.Rda",
  "math_piaac.Rda", "reading_piaac.Rda",
  "./02_results/cors.Rda",
  "./02_results/deltas.Rda"
), load, .GlobalEnv)

```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

In today’s knowledge-based and technology-rich societies, literacy (i.e., the ability to understand, use, and interpret written text) and numeracy (i.e., the ability to access, use, and interpret mathematical information)^[Some authors refer to literacy skills  as "reading competence" and to numeracy skills as "mathematical competence". There is no terminological consensus, and we use the term “skills”, "competencies", and "proficiencies" interchangeably throughout this article.] are quintessential skills for the welfare and well-being of individuals and societies at large. Literacy and numeracy skills have causal links to important individual (e.g., income, health, and social participation) and societal outcomes [e.g. economic growth; @hanushek2015; @hanushek2015a].
 
Demographic aging in most industrialized societies and rapid technological advances imply that the adult workforce will be increasingly required to update their skills throughout the life course, often well into the sixth decade of life. This trend toward lifelong learning in an aging workforce poses a number of important questions concerning the development of literacy and numeracy skills in adulthood. Because literacy and numeracy are key prerequisites to acquiring more specific knowledge and skills (e.g., reading a machine's manual or programming a computer) and not all individuals attain sufficient levels of literacy during schooling age [@durda2020], the key question is whether literacy and numeracy skills can still change during adulthood—or are  set like plaster after childhood and youth?  Moreover, does change in literacy and numeracy skills involve only losses - or do some adults experience proficiency gains? Moreover, how is skill change distributed in the adult population and in major subgroups such as age groups, genders, and different educational strata? 

These are not only interesting research questions in their own right, they are also of fundamental importance to policymakers and practitioners interested in promoting lifelong learning. If skills proved to be fully impervious to change during adulthood, investments in adults’ basic skills such as literacy and numeracy during adulthood might have a low return on investment, and childhood may be a more promising life stage for policies and interventions to focus on (e.g., @cunha2007). Alternatively, skills may change over time, but gains and losses may be unevenly distributed. In this case, identifying segments of the population that are more likely to experience age-related skill loss than others may aid the development of targeted policies and interventions. 

## Previous Evidence on Age Differences in Skills  
Given the policy relevance of these questions about skill development during adulthood, what answers can extant research provide us? Three key insights offered by current evidence are readily summarized [for comprehensive reviews, see @desjardins2012; @paccagnella2016]: (1) Lifelong plasticity of skills, (2) life stage dependency of change, and (3) individual and subgroup differences in change.

First, literacy and numeracy appear to continue to develop across adulthood. Both cross-sectional and the few available longitudinal studies suggest that these skills are not set like plaster after childhood but continue to change across the entire lifespan. Skill change during adulthood may involve both gains and losses. Second, gains and losses typically occur at different ages. As studies using data from international large-scale comparisons such as the Programme for the International Assessment of Adult Competencies (PIAAC) show, the cross-sectional age profile of literacy and numeracy follows an inverted u-shape: On average, literacy and numeracy skills continue to increase throughout the second decade of life, peak at around an age of 30 years, and gradually decline thereafter [e.g., @gabrielsen2014; @paccagnella2016; @podolskiy2014]. The resulting age differences in skills are substantial: On average across participating countries, older adults (aged 55–65 years) score about 30 scale points lower on the PIAAC literacy scale (the equivalent of 0.8 SD) than young adults aged 25–34 years [@paccagnella2016]. Third, beyond these average age trends, there are additional individual and group differences in  skill change. For example, using cross-sectional PIAAC 2012 data, @paccagnella2016 compared age-related age differences in literacy and numeracy skills among adults with different levels of educational attainment (i.e., primary, secondary, and territory or above) and found that those with the highest educational attainment experienced slightly larger skill loss during adulthood than those with lower educationsl attainment.

Although these prior studies have greatly advanced our knowledge about skill development in adulthood, they cannot conclusively answer the questions posed at the outset. This is because these studies are overwhelmingly based on large-scale cross-sectional surveys such as PIAAC and its predecessors (i.e., the International Adult Literacy Survey in 1994 and 1998 or the Adult Literacy and Life Skills Survey in 2003, 2006, and 2008); or on small-scale longitudinal studies based on selective samples such as the longitudinal study of adult learners (LSAL) that focuses on  high-school dropouts in the US [@reder2009].  Cross-sectional studies into the age-related changes in literacy and numeracy skills are limited in that they are unable to disentangle age effects from cohort effects. That is, they are unable to ascertain whether the putative age differences are due to age-related changes or stem from preexisting differences in skills during childhood. Small-scale longitudinal studies based on selective samples, on the other hand, are limited in that their findings may not generalize to the population as a whole. Moreover, by their very nature, these studies cover some subgroups (e.g., high-school dropouts) in a certain life stage (e.g., young adulthood)–but not others subgroups (e.g., the highly educated) and life stages (e.g., old age) that may be of equal interest to policymakers and practitioners. Also, compared to literacy, the life-span development numeracy has received much less attention by prior research, despite arguments in the literature that numeracy skills are gaining in importance on today’s labor markets [e.g., @gal2020; @gauly2020].

In order to overcome the limitations of cross-sectional and small-scale longitudinal designs, repeated measures of literacy and numeracy skills are needed. Such data have long been in short supply. Until very recently, there were simply no data sources available internationally that combined  the following desirable features that would allow for complete and robust answers to questions surrounding age-related changes in skills during adulthood: A large and non-selective sample; objective, and high-quality skill assessments; and a repeated measures design.

## The Present Research 
In the present study, we leverage the unique analytical potential of two recent German large-scale assessment surveys that do meet the above criteria: PIAAC-longitudinal (PIAAC-L), a follow-up to the 2012 Programme for the International Assessment of Adult Competencies (PIAAC) study in Germany; and Starting Cohort 6 from the National Educational Panel Study (NEPS). Both surveys offer repeated, high-quality––and largely comparable––measures of adults' literacy and numeracy spaced three years (PIAAC-L) to six years (NEPS) apart. Combining these data offers us unprecedented opportunities to analyze age-related changes in literacy and numeracy skills during adulthood, allowing us to present what appear to be the most comprehensive descriptive analyses of age-related changes of literacy and numeracy skills during adulthood to date.

With these data, we seek to answer two guiding questions about the stability and change of adults' literacy and numeracy skills. First, to what extent do these skills change across the three- to six-year periods covered by PIAAC-L and NEPS? Second, does the extent of age-related change differ across major sociodemographic groups? 

We approach these questions from two perspectives on age-related change enabled by the repeated-measures designs [e.g., @deary2014]: (1) relative change as captured by the correlation between literacy or numeracy skills measured at two time points, $r_{T1, T2}$; and (2) absolute change as capture by the change score, $\Delta_{T1, T2}$ (for details, see Method). We present descriptive analyses of change both for the total populations as well as in major sociodemographic subgroups defined by age, gender, and educational attainment. These subgroup analyses allow us to detect potential sociodemographic gradients in literacy and numeracy development. Of particular interest to our study is the question whether the age differences in skills and subgroup differences therein that we obtain through our repeated-measures designs parallel, or differ from, estimates of age differences obtained in cross-sectional designs [e.g., those reported by @paccagnella2016]?

# Method
For full transparency, the completely reproducible R code for this study is available from the first author's github repository at [blinded for review]. The PIAAC-L and NEPS data upon which this code builds are are available upon registration from  https://www.gesis.org/en/piaac/rdc/data/piaac-longitudinal and https://www.neps-data.de/, respectively.

## Data and Samples

### PIAAC/PIAAC-L
 Our first data source is the German PIAAC/PIAAC-L study [version 3-0-0; @gesis2017]. PIAAC was conducted in 2012 and provides internationally comparable data on the skills of the working-age population (16–65 years) residing in private households in large number of (mainly OECD) countries [@oecd2013a]. In Germany, a registry-based sampling design was implemented, in which respondents were randomly sampled from local population registers in randomly selected German municipalities. A total of 5,465 interviews in respondents’ homes were achieved. At the end of the PIAAC interview, all German PIAAC respondents were asked whether they were willing to be re-contacted for a follow-up study (i.e., PIAAC-L) in the future. A total of 3,758 (or 69%) of the original 5,465 PIAAC 2012 respondents consented to being re-interviewed and could be successfully contacted for the first follow-up wave in 2014. 
Literacy and numeracy skills were assessed in PIAAC in 2012 (henceforth "$T_1$") and again three years later in PIAAC-L wave 2 in 2015 (henceforth "$T_2$"). Our longitudinal sample contains the `r (nrow(reading_piaac) / max(reading_piaac$.imp)) %>% printnum(., digits = 0)` adults aged 18-65 at $T_1$ for who took the test at both measurement occasions. For the longitudinal analyses, we multiplied the total population weight of PIAAC 2012 with a factor that accounts for non-response in PIAAC-L 2015. For more details on the samples, procedures and weighting, see the technical reports to PIAC 2012 Germany and PIAAC-L [@zabal2014; @zabal2017].  Table 1 (right column) shows the (unweighted) sociodemographic characteristics of the PIAAC-L sample at $T_1$.

### NEPS
Our second data source is Starting Cohort 6 (Adults) of NEPS [version 11-0-0; @artelt2020]. NEPS is an ongoing large-scale, multi-cohort, longitudinal survey on educational trajectories in Germany [for a detailed description including sampling procedures, see, @blossfeld2011]. Starting Cohort 6 comprised a sample of initially 11,649 adults aged 22–65 years who were interviewed in up to nine annual waves so far. Literacy and numeracy skills were assessed twice, first in 2010/2011 (henceforth "$T_1$") and again six years later in 2016/2017 ("$T_2$"). Our longitudinal sample comprises `r printnum((nrow(reading_neps) / max(reading_neps$.imp)), digits = 0)` respondents who took the literacy test at both occasions and `r printnum((nrow(math_neps) / max(math_neps$.imp)), digits = 0)` respondents for numeracy. We used longitudinal weights (NEPS variable w_t456789_std) for individuals who continuously participated until $T_2$ (wave 9), which are computed by means of the longitudinal weight of the previous wave, the probability of being part of the used sample, and the likelihood of participating at $T_2$ [for details on the weighting procedure, see @hammon2018]. Akin to PIAAC, this weight adjusts for nonresponse<!--Check whether this is true--> Table 1 (right column) shows the (unweighted) sociodemographic characteristics of the NEPS sample at $T_1$

## Measures of Literacy and Numeracy
PIAAC/PIAAC-L and NEPS assessed literacy and numeracy in comparable ways [for a detailed comparison of the literacy assessments in PIAAC and NEPS, see @durda2020]. Both conceptualized literacy and numeracy from a functional perspective, with assessment items reflecting problems and tasks encountered in everyday life. The definitions and operationalizations of literacy and numeracy in these surveys correspond closely to  reading and writing ability (*Grw*)[^Note that the literacy tests focused on reading, not writing, and hence a subset of the narrow abilities covered by *Grw*.] and quantitative kowledge (*Gq*), respectively, in the updated Cattell-Horn-Carroll (CHC) model of intelligence [@mcgrew2009].

Despite some differences in the specifics of the assessment approaches (for details, see below), evidence from a linking study suggests high convergence between the PIAAC and NEPS tests [@carstensen2017]. As part of this linking study, a subset of participants of Wave 2 of PIAAC-L (2015) received different test versions, some of which contained items from both PIAAC and NEPS. This allows to jointly scale the test results using item response theory (IRT) methods. Results from this linking study show that the PIAAC and NEPS test correlate highly: The PIAAC literacy test correlates at *r* = .87 with the NEPS literacy test, and the PIAAC numeracy test correlates at *r* = .90 with the NEPS numeracy test. Moreover, the correlations between the literacy and numeracy tests within each study were also very similar: PIAAC literacy correlated with PIAAC numeracy at *r* = .87, and NEPS literacy correlated with NEPS numeracy at *r* = .87. This pattern of correlations suggests that the tests measure largely the same latent constructs.

It is also important to note that the assessments in both surveys were low-stakes assessment. Thus, the test scores are likely to reflect typical, rather than maximum performance, although interviewers took steps to ensure that respondents took the test situation seriously and invested sincere efforts to resolve the items. 

### PIAAC assessment approach
In PIAAC and PIAAC-L, literacy and numeracy skills were assessed through comprehensive, extensively validated (including cross-nationally) tests. Test tasks were devised by an international commission of eminent scholars [@expert2009a; @expert2009b]. The tasks were designed to reflect tasks relevant to everyday life , which respondents were typically highly motivated to solve. Respondents either took a computer-based assessment (86.2% in our analysis sample in 2012; 88.1% in 2015) or, if they had no experience using a computer, on a paper-pencil version (13.8% in 2012 and 11.8% in 2015). Depending on whether respondents took the computer-based assessment or paper-based assessment, they received different testlets that comprised different subsets of the test tasks. The computer-based assessment was a multistage adaptive testing design that consisted of different literacy and numeracy testlets (each comprising 20 tasks) to which respondents were assigned. The paper-based assessment comprised 20 fixed items for literacy and for numeracy. Interviewers were thoroughly trained for the assessment, they were present while respondents took the tests, and monitored the process. There was no time limit (i.e., tests were not speeded). On average, respondents took about 50 minutes to complete the assessment. For further information on the PIAAC assessment frameworks, see @oecd2012.

### NEPS assessment approach. 
In NEPS, literacy (or, in NEPS terminology, “reading competence”) and numeracy (“mathematical competence”) were assessed through a paper-pencil-based assessment at $T_1$ and a multistage adaptive computer-based assessment at $T_2$ Much akin to PIAAC, the tests were designed to measure skills needed in everyday life. Different to PIAAC, the tests were speeded. The $T_1$ literacy test comprised 32 items, which respondents were asked to complete in 28 minutes. The $T_1$ numeracy test comprised 22 items for which another 28 minutes were available. Respondents were randomly assigned different booklets, and not all respondents took both tests; however, all respondents received all items from a given booklet. At $T_2$, by a multistage design was employed , with respondents receiving one of two booklets of varying difficulty levels depending on their previous performance at $T_1$ (literacy) or their performance in an initial block of tasks at $T_2$ (numeracy). The $T_2$ literacy booklets comprised 27 or 26 items and the numeracy booklet comprised 21 items.  For further information on the NEPS assessment frameworks, see @gehrer2013 for literacy and @neumann2013 for numeracy.

### Plausible values (PV)
Literacy and numeracy skills are latent variables that cannot be observed directly but only inferred from individuals' responses to a series of test items. Point estimates of individual abilities (i.e., "test scores") such as number-right scores or weighted likelihood estimates [WLE; @warm1989], therefore, always contain some measurement error. Measurement error can bias both Rank-order consistencies (which are typically attenuated) and mean-level change (which can be over- or underestimated, depending on the method used and the extremity of an individual's score). To avoid such biases, we used plausible value (PV) methodology in both studies. PV methodology is currently the gold standard in international large-scale assessments. Instead of estimating a single test score per individual, PV appropriately accounts for the uncertainty about each individual's true skill. It does so drawing multiple, equally "plausible" values of each individual's skills based on a model which includes responses to test items, as well as a large set of background variables (e.g., age, education, employment, motivation, reading practice, and many more). Conceptually, the different plausible values are multiple imputations of the missing skill score for each person and can be analyzed by standard multiple imputation (MI) methodology. The crucial advantage of PV for our present intent is that they allow for unbiased estimates of population quantities (including our two key metrics of change, $\Delta_{T_1,T_2}$ and $r_{T_1,T_2}$) that are corrected for measurement error [for further details on PV methodology, see @wu2005 and @vondavier2009; for a non-technical introduction to the usage and advantages of PVs over more traditional methods, see @lechner2020].

The PIAAC/PIAAC-L data provides ten PVs per respondent and per skill domain and measurement occasion based on item response theory models (IRT) with an extensive background model comprising numerous variables. These variables provide additional information about a person's likely literacy and numeracy proficiency and helps improve precision of the PVs. The NEPS data also includes 10 PVs per respondent. However, because the background model only includes a minimal set of variables, we estimated a custom set of `r reading_neps %>% summarize(n_imp = max(.imp))` plausible values based on a more comprehensive background model via the NEPSscaling package [@scharl2020].^[This background model included age, age squared and age cubic, gender, educational attainment (ISCED levels), mother tongue, household size, federal state of Germany, gross monthly income, town size, number of books in the household cumulative employment duration across the lifespan, father's SES, meta-cognition in the same domain (i.e., math or reading),  the competence levels in the other domain at both T~1 and T~2, fluid intelligence (reasoning ability), science competence, and ICT competence. For further details, see the R code for PV estimation for this paper at https://github.com/CleLec/scacom/blob/master/01_pv_estimation_neps.R.]. Although even a single PV would allow for unbiased estimates and most studies offer 10 PVs (as in PIAAC), a higher number of PVs improves precision and reduces standard errors.
In both datasets, we ran each of our analysis (described below) separately on each of the plausible values and aggregated the results according to Rubin's rules [@rubin1987]. 

## Analyses
## Metrics of Change
As noted at the outset, we compute two complementary indices to gauge the extent of relative and absolute change in literacy and numeracy skills, described below [e.g., @deary2014. We compute each of these three indices for the total population as well as separately by age, educational attainment, and gender. This allows us to detect potential sociodemographic gradients in skill development.

Our first metric of change is based on the change (or difference) score between two measurement occasions, $\Delta_{i, T_1, T_2} = y_{i,T_2} - y_{i,T_1}$. By aggregating the change score across all individuals in the sample or a subgroup, we obtain our estimate of interest, mean-level change in the population or subgroup in question:
$$\Delta_{T_1, T_2} = \sum_{i=1}^{n} \frac{y_{i, T_2} - y_{i, T_2}}{n} $$ 

This is the most straightforward and widely used metric of change in an outcome in developmental research. Note that values of $\Delta_{T_1,T_2}$ are in the raw metric of the tests (0–500 points in PIAAC; logits in NEPS). Therefore, to enhance comparability within and across studies, we report the standardized effect size Cohen’s $d_{av}$, which expresses the average (mean-level) change in skills over in units of the standard deviation of the test. As is conventional [@lakens2013], we used the pooled standard deviation from both time points as an estimate of:
$$d_{av} = \frac{\Delta_{T_1, T_2}}{\sigma_{pooled}} = \frac{\sum_{i=1}^{N} \frac{y_{i, T_2} - y_{i, T_2}}{N}}{0.5 \times (SD_{T_1} + SD_{T2})}$$

Our second metric of change was the rank-order consistency [sometimes called "differential stability"; e.g., @schalke2013] of individuals’ skills across two measurement occasions. Its measure is the Pearson correlation between the tests at two time points,
$$r_{T_1,T_2} = \frac{\sum_{i=1}^{N} (y_{i,{T_1}} - \overline{y}_{T_1}) \times (y_{i,{T_2}} - \overline{y}_{T_2}) }{\sigma_{T_1}\sigma_{T_2}}$$
, where $Y$ refers to the literacy or numeracy proficiency of an individual $i$ at time $T$. Rank-order consistency is a widely used standardized effect size measure that can be readily compared within and across studies.

The two metrics of change offer complementary information: Whereas $r_{T_1,T_2}$ refers to how stable individuals’ relative standing in the skill distribution is across two measurement occasions [i.e., the stability of individual differences; @deary2014],  $\Delta_{T_1,T_2}$ refers to the how much individuals’ skills change over time in absolute terms. Absolute change is largely independent of relative change: Even if there is hardly any rank-order change (e.g.,  $r_{T_1,T_2} > .90$), this does not preclude the possibility of substantial mean-level change—as long as skill gains or losses are very similar in size across individuals (i.e.,  $\Delta_{T_1,T_2}$ has little variability).

### Subgroup analyses
Our analyses of variation in stability and change of skills across sociodemographic subgroups required that we split the age and education variables in a way that was (a) theoretically meaningful; (b) fine-grained enough to detect potential differences while (c) resulting in subgroups large enough to ensure stable estimates of change in both NEPS and PIAAC-L. Based on these criteria, we chose to code age in four groups: young adults (18–34 years), prime-age adults (35–44 years), mid-aged adulthood (45–54 years) and older adults (55 years and older). We chose to code educational attainment in three groups according to the International Standard Classification of Education (ISCED 1997): lower or upper secondary (“low”; ISCED levels 0–3), post-secondary or tertiary vocational education (“intermediate”; ISCED levels 4, and 5B), and tertiary academic education (“high”; ISCED levels 5A and 6). For gender, we used a binary variable distinguishing men from women (note that PIAAC and NEPS did not yet include a diverse/third gender category).

# Results


## Preliminary analyses
Bevore 
- Correlations between literacy and numeracy in PIAAC
- Correlations between the measures of change (delta) in PIAAC and NPS

## Mean-level change
(ref:my-figure-caption) My caption.
```{r my-figure, fig.cap = "(ref:my-figure-caption)"}
plot(cars)
```

Figure 1 presents results for our first metric of change: mean-level change in literacy (blue) and numeracy (red) across three years (PIAAC-L, left panel) to six (NEPS, right panel) years of adulthood in the total population and in the subgroups, expressed in Cohen's $d_{av}$.

In PIAAC-L, there was little evidence for mean-level change  across three years in literacy in the total population. The same applied to numeracy. When looking at change separately in sociodemographic subgroups, few differences emerged. For literacy (but not numeracy), there were small gains among the most highly educated. Moreover,there were small gains in both literacy and numeracy in the youngest age group. 

Likewise, in NEPS, there was little evidence for mean-level change  across six years in literacy or numeracy in the total population. In contrast to PIAAC-L, the tendency of most effect sizes was towards losses, not gains, in literacy and numeracy. Subgroup differences were mostly minor: apart from a small gender difference in numeracy change favoring women, highly-educated individuals experienced sizeable losses in literacy and, to a lesser extent, numeracy over the six-year period. The losses in literacy exceeded the definition of a Moreover, there was a tendency (although less pronounced than in PIAAC) for gains in the youngest and losses in the older age groups, although again the statistical uncertainty about these estimates was considerable. 

However, despite these
### Change across three years in PIAAC-L 


## Rank-order consistencies
The impression fom

```{r load_results}
# Seed for random number generation
library(tidyverse)
library(mitools)
library(extrafont)
load("math_neps.Rda")
load("reading_neps.Rda")
load("math_piaac.Rda")
load("reading_piaac.Rda")

change_neps <- full_join(
  select(reading_neps, .imp, id = ID_t, t1_read = t1_pv, t2_read = t2_pv, weight),
  select(math_neps, .imp, id = ID_t, t1_math = t1_pv, t2_math = t2_pv),
  by = c("id", ".imp")
) %>%
  mutate(diff_read = t2_read - t1_read,
         diff_math = t2_math - t1_math) %>%
  select(.imp, diff_read, diff_math, t1_read, t2_read, weight) %>%
split(.$.imp) %>% 
 imputationList()

change_neps <- with(change_neps, lm(scale(diff_read) ~ scale(diff_math),
                   weights = weight
    )
  ) %>%
    MIcombine() %>%
    summary()

change_piaac <- full_join(
  select(reading_piaac, .imp, id = seqid, t1_read = t1_pv, t2_read = t2_pv, weight),
  select(math_piaac, .imp, id = seqid, t1_math = t1_pv, t2_math = t2_pv),
  by = c(".imp", "id")
) %>%
  mutate(diff_read = t2_read - t1_read,
         diff_math = t2_math - t1_math) %>%
  select(.imp, diff_read, diff_math, t1_read, t2_read, weight) %>%
split(.$.imp) %>% 
 imputationList()

change_piaac <- with(change_piaac, lm(scale(diff_read) ~ scale(diff_math),
                   weights = weight
    )
  ) %>%
    MIcombine() %>%
    summary()

change_piaac

```

# Discussion
Are literacy and numeracy skills set like plaster after schooling age – or do they continue to change throughout adulthood? If the latter, does change involve gains or losses, and how is it distributed in the population? The present paper used some of the best available data to answer these question: the German PIAAC-L and NEPS studies, offering complementary information about change in adults' literacy and numeracy skills across a three-year (PIAAC-L) to six-year (NEPS) period. 

Our analyses yielded three main insights. First, on average, literacy and numeracy skills change very little across a period of about half a decade covered by the two studies. There was a tendency toward gains in literacy but not numeracy  in the total population in PIAAC-L and a  tendency toward losses in both competences in NEPS, yet effect sizes hovered around zero for both skills and in both studies (`r deltas %>% filter(grepl("total", grouping)) %>% select(dav) %>% min() %>% printnum() ` $\geq d \geq$ `r deltas %>% filter(grepl("total", grouping)) %>% select(dav) %>% max() %>% printnum()`). 

Second, however, we found a considerable degree of individual differences behind these negligible mean-level changes in skills. Specifically, the change scores $\Delta_{T_1, T_2}$ were approximately normally distributed, implying that gains and losses in literacy and numeracy over time were nearly equally prevalent in these samples. Further adding to this picture, the rank-order consistencies in the total population $r_{T_1, T_2}$ were high but not perfect across the three-year period in PIAAC (`r cors %>% filter(grepl("piaac", target) & grepl("total", grouping)) %>% select(rho) %>% min() %>% printnum() ` $\geq r \geq$ `r cors %>% filter(grepl("piaac", target) & grepl("total", grouping)) %>% select(rho) %>% max() %>% printnum()`) and only moderate across the six-year period in NEPS (`r cors %>% filter(grepl("neps", target) & grepl("total", grouping)) %>% select(rho) %>% min() %>% printnum() ` $\geq r \geq$ `r cors %>% filter(grepl("neps", target) & grepl("total", grouping)) %>% select(rho) %>% max() %>% printnum()`). Literacy was significantly less rank-order consistent than numeracy in NEPS, yet this difference did not replicate in PIAAC where, if anything, literacy appeared slightly more consistent.

To put these rank-order consistencies into perspective, several studies on cognitive development and aging found higher rank-order consistencies for measures of intelligence over longer periods of time. For example, @gow2011, in one of the few longitudinal studies on the stability of cognitive ability across the life span, found that general intelligence (*g*) measured with the Moray House Test No. 12 (MHT) at age 11 correlated at $r = .67$ ($r = .78$ after disattenuation) with intelligence at age 70 in 1,017 Scottish individuals from the Lothian Birth Cohort study. Other cohort studies from Scotland yielded comparable estimates but found rank-order consistencies to decline somewhat as the age of individuals at retest further increased [@deary2014]. Similarly, @schalke2013 reported rank-order consistencies of $r = .85$ for *g* from age 12 to age 52, with only slightly lower estimates for more specific abilities (e.g., fluid reasoning, comprehension knowledge, visual processing) among 344 individuals from Luxembourg. One should bear in mind that the measures of intelligence in these studies are not directly comparable to the measures of literacy and numeracy in NEPS [recall that literacy and numeracy are best viewed as broad abilities on Stratum II in the CHC model of intelligence; @mcgrew2014].  Still, it is clear that the rank-order consistencies of literacy and numeracy over three to six years in PIAAC-L and NEPS are relatively low in comparison to that of general intelligence in these studies. 

Third, although there was considerable interindividual variation in skill change over time, few of these differences were systematic across sociodemographic subgroups. Most of the subgroup differences in mean-level change were small and, even more important, did not replicate across the two studies and time-periods. In fact, some of the subgroup differences were in outright opposite direction in the two studies: The gender differences in mean-level change observed in NEPS were not present in PIAAC-L; the age differences in PIAAC-L replicated the well-known pattern from cross-sectional data (gains in younger adults, no change or losses in older adults) only partly replicated in NEPS; and the literacy gains of higher-educated individuals in PIAAC-L were starkly contrasted by losses in NEPS. The losses in literacy skills among the  highly educated in NEPS was the strongest mean-level difference we observed (*d* = `r deltas %>% select(delta) %>% abs() %>% max()`); change of about a fourth of a standard deviation is sizeable, given the relatively short period of six years. The latter result may be indicative of ceiling effects, combined with regression to the mean, in the NEPS assessment compared to PIAAC. Analyses investigating literacy skill change in NEPS separately by initial skill level [@durda2020] corroborated this interpretation, pointing to regression to the mean. Moreover,  controlling for initial skill levels in regression models leads to the conclusion that higher-educated individuals in NEPS experience relative gains, not losses, in literacy, resembling a "Matthew effect" [@wicht2020]. 

Subgroup differences in rank-order consistencies were no more pronounced than those in mean-level stability. The only meaningful difference we found was the lower rank-order consistency of literacy (but not numeracy) in the oldest compared to the younger age groups in NEPS (but not PIAAC). Because of the limited sample size in these subgroups, the statistical uncertainty about these difference is hihgh, however. Thus, we conclude that sociodemographic subgroups do not differ strongly in change and stability of literacy and numeracy skills. 

## Limitations and Directions for Future Research 
The major advancement made by our study was to approach the question of change and stability in adults' literacy and numeracy skills through the lens of repeated-measures data *and* large-scale, non-selective samples, and to apply PV methodology [@wu2005; @vondavier2009] to account for measurement error in skills . This combination has so far been extremely rare, if not completely absent from the literature, as previous evidence on age-related changes in literacy and numeracy skills hails either from large-scale cross-sectional or  small-scale (and often highly selective) longitudinal studies [@paccagnella2016; @desjardins2012]. We are not aware of any comparably rich data sources on adults' skills internationally<!-- Britta, würdest Du das unterschreiben?-->. Still, our study leaves several questions unanswered that future research should address. 

First, our data covered only two measurement occasions spanning time periods of three to six years. As our analyses show, even such relatively limited time periods allow for change in literacy and numeracy to occur. Nonetheless, six years represent a mere `r 6 / 80.99 *100`% of the current average life span of an individual born in Germany. Hence, future studies following individuals over longer time periods and assessing literacy and numeracy at multiple time points would enable more nuanced insights into the temporal dynamics of change in literacy and numeracy. Of note, it is not necessarily the case that rank-order consistencies are lower for longer time intervals, and it will be interesting how the stability of literacy and numeracy plays out in the long-run. So far, however, no truly large-scale, long-running panel studies measuring adults' literacy and numeracy skills in the sense of broad competencies[^Note there is handful of longitudinal studies on more narrow, circumscribed aspects of intelligence from research on aging [for discussions, see @deary2014]. However, these studies rarely measure broad competence domains such as literacy and numeracy, and some of them do not conform with modern psychometric standards.] at multiple time points are available, even though NEPS is ongoing and will continue to grow in this regard. 

Second, as noted, the assessments in PIAAC and NEPS were not high-stakes assessment. Although interviewers in both studies did their best to ensure that respondents took the tests seriously, the tests are more likely to represent "typical" as opposed to "maximum" performance. Whether typical or maximum performance in more relevant for life outcomes can be debated, yet is clear that high-stakes assessments might lead to different estimates of stability and change. In high-stakes assessment, individuals are more likely to show their best possible  minimize the influence of extraneous influences on test per  <!--Hierzu hat Silke was in ihrem Papier geschrieben--> 

Third, it will be important to understand how other  assessment approaches influence estimates of stability and change in adult competencies. of PIAAC-L and NEPS 


## Conclusion
Together, these individual differences in mean-level change and rank-order consistencies suggest that despite the seemingly immutable population means, a substantial amount of change in literacy and numeracy over time occurred in some individuals. This change comprised gains and losses in almost equal share such that gains and losses canceled each other out when looking only at mean-level change. 

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>

\endgroup
