---
title             : | 
  Stability and Change in Adults' Literacy and Numeracy Skills:
  Evidence From two Large-Scale Panel Studies
shorttitle        : "Stability and Change in Adults' Literacy and Numeracy"

author:
  - name          : "Clemens M. Lechner"
    affiliation   : "1"
    corresponding : yes    
    address       : "B2, 1, 68159 Mannheim, Germany"
    email         : "clemens.lechner@gesis.org"
    role:          
      - Funding acquisition
      - Supervision
      - Conceptualization
      - Data curation
      - Writing - Original Draft 
      - Writing - Review & Editing
      - Methodology 
  - name          : "Britta Gauly"
    affiliation   : "1"
    role:
      - Data curation
      - Writing - Review & Editing
  - name          : "Ai Miyamoto"
    affiliation   : "2"
    role: 
      - Writing - Review & Editing
  - name          : "Alexandra Wicht"
    affiliation   : "3"
    role:
      - Data curation
      - Formal analysis
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "GESIS -- Leibniz Institute for the Social Sciences, Mannheim, Germany"
  - id            : "2"
    institution   : "University of Freiburg, Germany"
  - id            : "3"
    institution   : "University of Siegen, Germany"

authornote: |
  The authors would like to thank student research assistants Marcel Kappes and Thomas Knopf, and William Thorn (OECD) for helpful comments on an earlier version of this study.

abstract: |
  This study asked how literacy and numeracy skills develop adulthood. We used data from two large-scale surveys: The PIAAC longitudinal study (PIAAC-L; *N* = 1,775) and the National Educational Panel Study (NEPS; *N* = 3,087), offering repeated measures of literacy and numeracy spaced three (PIAAC-L) to six (NEPS) years apart. We  examined mean-level change ($\Delta{T_1,T_2}$) and rank-order consistencies ($r_{T_1,T_2}$) in the population and in sociodemographic subgroups defined by age, gender, and education. Results revealed that literacy and numeracy are highly but not perfectly rank-order stable ($.66 \leq r \leq .83$). Mean-level change was near zero for both skills and studies, but there was considerable variation in the change scores across individuals. Subgroup differences in the two indices of change were mostly small and did not replicate across studies. This suggests that adults' literacy and numeracy are not set in stone and can change even over relatively short time periods. 
 
 
keywords          : "literacy, numeracy, development, adulthood, longitudinal"
wordcount         : "X"
bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : yes
tablelist         : no
footnotelist      : yes
linenumbers       : no
mask              : yes
draft             : no
csl               : "apa.csl"
#documentclass     : "apa6"
documentclass     : "apa7"
classoption       : "man"
output            : papaja::apa6_pdf
appendix          : "appendix.Rmd"
header-includes   :
  - \usepackage{rotating}
  - \DeclareDelayedFloatFlavor{sidewaysfigure}{figure}
 # - \usepackage[default]{sourcesanspro}
  - \usepackage[T1]{fontenc}

---


```{r setup, include = FALSE}
# Load required packages

library(tidyverse)
library(glue)
library(sjlabelled)
library(kableExtra)
library(extrafont) 
library(papaja)
library(mice)
library(miceadds)
library(mitools)
library(srvyr)
library(cowplot)

# BibLateX Reference file
r_refs("r-references.bib")

# Generate the tables and figures for reporting
source("04_reporting.R")

```

# Introduction
In today's knowledge-based and technology-rich societies, literacy (i.e., reading competence, or the ability to understand, use, and interpret written text) and numeracy (i.e., mathematical competence, or the ability to access, use, and interpret mathematical information) ^[\ NEPS refers to literacy skills as "reading competence" and to numeracy skills as "mathematical competence" [@weinert2011]. There is no terminological consensus, and we use the term "skills", "competencies", and "proficiencies" interchangeably throughout this article.] are quintessential skills for the welfare and well-being of individuals and societies. Literacy and numeracy have links to important individual such as income, health, and social participation as well as and macro-levle societal outcomes such as economic growth [e.g., @oecd2013; @oecd2016; @hanushek2015; @hanushek2015a].
 
Demographic aging in most industrialized societies and rapid technological advances imply that the adult workforce will be increasingly required to update their skills throughout the life course, often well into the sixth decade of life. Literacy and numeracy are key prerequisites to acquiring more specific knowledge and skills (e.g., reading a machine's manual or programming a computer), and hence for lifelong learning at large. Because not all individuals attain sufficient levels of literacy during schooling age [@durda2020; @wicht2021; @wolf2014], the key question is whether literacy and numeracy skills can still change during adulthood; and if so, whether such change would involve only losses -- or proficiency gains at least in some adults? Moreover, how is skill change distributed in the adult population and in subgroups such as age groups, genders, and educational strata? 

These are not only interesting research questions in their own right, they are also of importance to policymakers and practitioners interested in promoting lifelong learning [e.g., @wolf2014]. If literacy and numeracy proved to be impervious to change during adulthood, investments in basic skills or workplace learning programs aimed at fostering literacy and numeracy during adulthood might have a low return on investment, and childhood may be a more promising life stage for policies and interventions to focus on [e.g., @cunha2007]. Moreover, literacy and numeracy may change over time, but gains and losses may be unevenly distributed among (different groups of) individuals. In this case, identifying segments of the population that are more likely to experience skill loss than others may aid the development of targeted policies and interventions. 

## Previous Evidence on Age Differences in Skills 
Given the policy relevance of literacy and numeracy development during adulthood, what answers can extant research provide us? Three key insights offered by current evidence are readily summarized [for reviews, see @desjardins2012; @paccagnella2016; also see @deary2014]: (1) Lifelong plasticity of skills, (2) life stage dependency of change, and (3) individual and subgroup differences in change.

First, literacy and numeracy appear to continue to develop across adulthood. Despite the fact that literacy have a strong genetic component [i.e., are highly heritable; @andreola2020], both cross-sectional and the few available longitudinal studies suggest that these skills are not set like plaster after childhood but continue to change across the lifespan [@desjardins2012; @paccagnella2016; @oecd2016]. Skill change during adulthood may involve both gains and losses. 

Second, gains and losses typically occur at different ages. As studies using data from international large-scale comparisons such as the Programme for the International Assessment of Adult Competencies (PIAAC) show, the cross-sectional age profile of literacy and numeracy follows an inverted U-shape: On average, literacy and numeracy skills continue to increase throughout the second decade of life, peak at around an age of 30 years, and gradually decline thereafter [e.g., @gabrielsen2014; @paccagnella2016; @podolskiy2014; @oecd2016]. The resulting age differences in skills are substantial: On average across participating countries, older adults (aged 55-65 years) score about 30 scale points lower on the PIAAC literacy scale (the equivalent of $0.8\ SD$) than young adults aged 25-34 years [@paccagnella2016]. Research on cognitive aging focusing on more basic cognitive abilities has revealed similar patterns. For example, a recent large-scale study (e.g., processing speed, reasoning, memory) found approximately linear declines in cognitive abilities from early adulthood onward that accelerated in old adulthood [@salthouse2019], with the exception of increases in vocabulary -- a pattern generally consistent with the age trajectories of fluid (*Gf*) and crystallized (*Gc*) intelligence hypothesized by @cattell1971 in his seminal work. Literacy and numeracy skills as conceptualized in the present study are closer conceptually to *Gc* but also require *Gf*, such that declines in these competences with age could be expected, especially in old age. 

Third, beyond these average age trends, there are additional individual and group differences in skill change. For example, using cross-sectional PIAAC 2012 data, @paccagnella2016 compared age differences in literacy and numeracy skills among adults with different levels of educational qualification (i.e., primary, secondary, and territory or above) and found that those with the highest educational qualification experienced slightly larger skill loss during adulthood than those with lower educational qualification By contrast, a recent longitudinal study in German adults found that higher education had a protective effect against declines in literacy over time after controlling for initial level of literacy skills [@wicht2020]. In addition, some studies suggest a tendency of female adults having relative strength in literacy and of male adults having relative strength in numeracy across the life course [e.g, @houtkoop1998; @satherley2008]. It remains unclear whether this translates to differential change over time as other studies did but not find any pronounced gender differences in cross-sectional PIAAC skill profiles [@oecd2016] and in skill change over time [@reder2009; @wicht2020]. Another longitudinal study found women to start off at higher literacy levels but experience smaller proficiency gains over time compared to men [@wolf2014].

Although these prior studies have greatly advanced our knowledge about skill development in adulthood, they cannot conclusively answer the questions posed at the outset. This is because these studies are overwhelmingly based on large-scale cross-sectional surveys such as PIAAC and its predecessors (i.e., the International Adult Literacy Survey in 1994 and 1998 or the Adult Literacy and Life Skills Survey in 2003, 2006, and 2008); or on small-scale longitudinal studies based on selective samples such as the longitudinal study of adult learners (LSAL) that focuses on high-school dropouts in the US [@reder2009] or adults in basic skills programs in the UK [e.g., @wolf2014]. Cross-sectional studies can be advantageous in that they do not suffer from selective attrition of low-skilled or lower-educated individuals, as most longitudinal assessment surveys do [@martin2020]. Moreover, they are unbiased from retest artifacts which plague some longitudinal surveys, especially ones that apply designs in which the exact same items are administered repeatedly [@salthouse2019]. At the same time, cross-sectional studies of age differences are limited in that they are unable to disentangle age-related changes from cohort effects. That is, they are unable to ascertain whether the putative age differences are due to age-related changes or stem from preexisting differences in skills during childhood. Small-scale longitudinal studies based on selective samples, on the other hand, are limited in that their findings may not generalize to the population as a whole. Moreover, by their very nature, these studies cover some subgroups (e.g., high-school dropouts) in a certain life stage (e.g., young adulthood) but not others (e.g., the highly educated) and life stages (e.g., old age) that may be of equal interest to policymakers and practitioners. Also, compared to literacy, the life-span development numeracy has received much less attention by prior research, despite arguments in the literature that numeracy skills are gaining in importance on today's labor markets [e.g., @gal2020; @gauly2020]. 
In order to overcome the limitations of cross-sectional and small-scale longitudinal designs, repeated measures of literacy and numeracy skills are needed. Such data have long been in short supply. Until very recently, there were simply no data sources available internationally that combined the following desirable features that would allow for complete and robust answers to questions surrounding age-related changes in skills during adulthood: A large and non-selective sample; objective, and high-quality skill assessments; and a repeated measures design.

## The Present Research
In the present study, we leverage the unique potential of two recent German large-scale assessment surveys that do meet the above criteria: PIAAC-longitudinal (PIAAC-L), a follow-up to the 2012 Programme for the International Assessment of Adult Competencies (PIAAC) study in Germany; and Starting Cohort 6 from the National Educational Panel Study (NEPS). Both surveys offer repeated, high-quality -- and largely comparable -- measures of adults' literacy and numeracy spaced three (PIAAC-L) to six years (NEPS) apart. Combining these data offers us unprecedented opportunities to analyze changes in literacy and numeracy skills over time during adulthood, allowing us to present what appear to be the most comprehensive descriptive analyses of age-related changes of literacy and numeracy skills during adulthood to date.  
With these data, we seek to answer two guiding questions about the stability and change of adults' literacy and numeracy skills. First, to what extent do these skills change across the three- to six-year periods covered by PIAAC-L and NEPS? Second, does the extent of age-related change differ across major sociodemographic groups?  
We approach these questions from two perspectives on age-related change enabled by the repeated-measures designs [e.g., @deary2014]: (1) relative change as captured by the correlation between literacy or numeracy skills measured at two time points, $r_{T1, T2}$ (rank-order consistency); and (2) absolute change as capture by the change score, $\Delta{T1, T2}$ (mean-level change; for details, see Method). We present descriptive analyses of change both for the total populations as well as in major sociodemographic subgroups defined by age, gender, and educational qualification These subgroup analyses allow us to detect potential sociodemographic gradients in literacy and numeracy development. Of particular interest to our study is the question whether the age differences in skills and subgroup differences therein that we obtain through our repeated-measures designs parallel, or differ from, estimates of age differences obtained in cross-sectional designs [e.g., those reported by @paccagnella2016; @deary2014]?

# Method and Materials
## Data and Samples 
### PIAAC/PIAAC-L
 Our first data source is the German PIAAC/PIAAC-L study [version 3-0-0; @gesis2017]. PIAAC was conducted in 2012 and provides internationally comparable data on the skills of the working-age population (16-65 years) residing in private households in large number of (mainly OECD) countries [@oecd2013a]. In Germany, a registry-based sampling design was implemented, in which respondents were randomly sampled from local population registers in randomly selected German municipalities. A total of 5,465 interviews in respondents' homes were achieved. At the end of the PIAAC interview, all German PIAAC respondents were asked whether they were willing to be re-contacted for a follow-up study (i.e., PIAAC-L) in the future. Of the 5,465 respondents who had completed the PIAAC 2012 assessment (henceforth "$T_1$"), 3,758 consented to being re-interviewed. A total of 3,263 participated in the second wave of PIAAC-L 2015 in which literacy and numeracy were re-assessed (henceforth "$T_2$"). After including minors (younger than 18 years), our longitudinal sample contains the `r (nrow(reading_piaac) / max(reading_piaac$.imp)) %>% printnum(., digits = 0)` adults aged 18--65 at $T_1$ for who took the test at both measurement occasions. For the longitudinal analyses, we multiplied the total population weight of PIAAC 2012 with a factor that accounts for non-response in PIAAC-L 2015. For more details on the samples, procedures and weighting, see the technical reports to PIAAC 2012 Germany and PIAAC-L [@zabal2014; @zabal2017]. Table\ \@ref(tab:table1) (right column) shows the sociodemographic characteristics and skill profile of the (unweighted) PIAAC-L sample.

### NEPS
Our second data source is Starting Cohort 6 (Adults) of NEPS [version 11.0.0; @artelt2020]. NEPS is an ongoing large-scale, multi-cohort, longitudinal survey on educational trajectories in Germany [for a detailed description including sampling procedures, see, @blossfeld2011]. Starting Cohort 6 comprised a sample of initially 11,649 adults aged 22--65 years who were interviewed in up to nine annual waves so far. Literacy and numeracy skills were assessed twice, first in 2010/2011 (henceforth "$T_1$") and again six years later in 2016/2017 ("$T_2$"). Our longitudinal sample comprises `r printnum((nrow(reading_neps) / max(reading_neps$.imp)), digits = 0)` respondents who took the literacy test at both occasions and `r printnum((nrow(math_neps) / max(math_neps$.imp)), digits = 0)` respondents for numeracy. We used longitudinal weights (NEPS variable w_t456789_std) for individuals who continuously participated until $T_2$ (wave 9), which are computed by means of the longitudinal weight of the previous wave, the probability of being part of the used sample, and the likelihood of participating at $T_2$ [for details on the weighting procedure, see @hammon2018]. Akin to PIAAC, this weight adjusts for nonresponse (dropout). Table 1 (right column) shows the sociodemographic characteristics and skill profile of the (unweighted) NEPS sample.


(ref:table1-caption) Sociodemographic characteristics of the two samples and skill profile of the two samples. 

```{r table1, echo = FALSE}

# This relies on the tables created in 04_reporting.R

apa_table(
  rbind(
  data_descriptives, data_skills
  ),
  col_spanners = list("\\makecell[c]{PIAAC-L\\\\(\\emph{N} = 3,087)}" = c(2,4),                       "\\makecell[c]{NEPS\\\\(\\emph{N} = 1,775)}"  = c(5,7)),
    #"NEPS (\\emph{N} = 1,775)" = c(5,7)),
  stub_indents = list("Age" =  c(1:5),
                      "Education" = c(7:9),
                      "Literacy" = c(10:12),
                      "Numeracy" = c(13:15)),
  escape = F,
  align = "lcccccc",
  caption = "(ref:table1-caption)",
  font_size = "small", 
  note = glue("Unweighted descriptive statistics. The socio-demographic characteristics in NEPS are for the literacy sample; the NEPS numeracy sample was slightly smaller (N = {printnum((nrow(math_neps) / max(math_neps$.imp)), digits = 0)}) but had virtually the same sociodemographic profile. For literacy and numeracy skills, cell values represent pooled estimates across the 10 (20) plausible values in PIAAC-L (NEPS), respectively. " )
)

```

## Measures of Literacy and Numeracy
PIAAC/PIAAC-L and NEPS assessed literacy and numeracy in comparable ways [for a detailed comparison of the literacy assessments in PIAAC and NEPS, see @durda2020]. Both conceptualized literacy and numeracy from a functional perspective, with assessment items reflecting problems and tasks encountered in everyday life. The definitions and operationalizations of literacy and numeracy in these surveys correspond closely to reading and writing ability (*Grw*) ^[\ Note that the literacy tests focused on reading, not writing, and hence a subset of the narrow abilities covered by *Grw*.] and quantitative kowledge (*Gq*), respectively, in the updated Cattell-Horn-Carroll (CHC) model of intelligence [@mcgrew2009].

Despite some differences in the specifics of the assessment approaches (for details, see below), evidence from a linking study suggests high convergence between the PIAAC and NEPS tests [@carstensen2017]. As part of this linking study, a subset of participants of Wave 2 of PIAAC-L (2015) received different test versions, some of which contained items from both PIAAC and NEPS. Joint item response theory (IRT) scaling of the tests showed that the PIAAC literacy test correlates at $r = .87$ with the NEPS literacy test, and the PIAAC numeracy test correlates at $r = .90$ with the NEPS numeracy test. The correlations between the literacy and numeracy tests within each study were also very similar: PIAAC literacy correlated with PIAAC numeracy at $r = .87$, and NEPS literacy correlated with NEPS numeracy at $r = .87$. This pattern of correlations suggests that the tests measure largely the same latent constructs.

It is important to note that the assessments in both surveys were low-stakes assessment. Thus, the test scores are likely to reflect typical, rather than maximum performance, even though interviewers took steps to ensure that respondents took the test situation seriously and invested sincere efforts to resolve the items. 

### PIAAC assessment approach
In PIAAC and PIAAC-L, literacy and numeracy skills were assessed through comprehensive, extensively validated (including cross-nationally) tests. Test tasks were devised by an international commission of eminent scholars [@expert2009a; @expert2009b]. The tasks were designed to reflect tasks relevant to everyday life, which respondents were typically highly motivated to solve. Respondents either took a computer-based assessment (86.2% in our analysis sample in 2012; 88.1% in 2015) or -- in case they were found to be unfit for the computer-based or refused to take it -- a paper-pencil version (13.8% in 2012 and 11.8% in 2015). Both assessments were scaled such that they are directly comparable [see @oecd2013b]. Depending on whether respondents took the computer-based assessment or paper-based assessment, they received different testlets that comprised different subsets of the test tasks. The computer-based assessment was a multistage adaptive testing design that consisted of different literacy and numeracy testlets (each comprising 20 tasks) to which respondents were assigned. The paper-based assessment comprised 20 fixed items for literacy and for numeracy. Interviewers were thoroughly trained for the assessment, they were present while respondents took the tests, and monitored the process. There was no time limit (i.e., tests were not speeded). On average, respondents took about 50 minutes to complete the assessment. @oecd2012 and @oecd2013 provide further information on the PIAAC assessment frameworks.  

### NEPS assessment approach  
In NEPS, literacy (or, in NEPS terminology, "reading competence") and numeracy ("mathematical competence") were assessed through a paper-pencil-based assessment at $T_1$ and a multistage adaptive computer-based assessment at $T_2$ Much akin to PIAAC, the tests were designed to measure skills needed in everyday life. Different from PIAAC, the tests were speeded. The $T_1$ literacy test comprised 32 items, which respondents were asked to complete in 28 minutes. The $T_1$ numeracy test comprised 22 items for which another 28 minutes were available. Respondents were randomly assigned different booklets, and not all respondents took both tests; however, all respondents received all items from a given booklet. At $T_2$, by a multistage design was employed, with respondents receiving one of two booklets of varying difficulty levels depending on their previous performance at $T_1$ (literacy) or their performance in an initial block of tasks at $T_2$ (numeracy). The $T_2$ literacy booklets comprised 27 or 26 items and the numeracy booklet comprised 21 items. For further information on the NEPS assessment frameworks, see @gehrer2013 for literacy and @neumann2013 for numeracy.  

### Plausible values (PVs)
Literacy and numeracy skills are latent variables that cannot be observed directly but only inferred from individuals' responses to a series of test items. Point estimates of individual abilities (i.e., "test scores") such as number-right scores or weighted likelihood estimates [WLE; @warm1989], therefore, always contain measurement error. Measurement error can bias both rank-order consistencies (which are typically attenuated) and mean-level change (which can be over- or underestimated, depending on the method used and the extremity of an individual's score). To avoid such biases, we used plausible value (PV) methodology in both studies. PV methodology is currently the gold standard in international large-scale assessments. Instead of estimating a single test score per individual, PV appropriately accounts for the uncertainty about each individual's true skill by drawing multiple, equally "plausible" values of each individual's skills based on a model which includes responses to test items, as well as a large set of background variables (e.g., age, education, employment, motivation, reading practice, and many more). Conceptually, the different plausible values are multiple imputations of the missing skill score for each person and can be analyzed by standard multiple imputation (MI) methodology. The crucial advantage of PV for our present intent is that they allow for unbiased estimates of population quantities (including our two key measures of change, $\Delta{T_1,T_2}$ and $r_{T_1,T_2}$) that are corrected for measurement error [for further details on PV methodology, see @wu2005; @vondavier2009; for a non-technical introduction to the usage and advantages of PVs over more traditional methods, see @lechner2020].

The PIAAC/PIAAC-L data include ten PVs per respondent, skill domain and measurement occasion based on item response theory models (IRT) based on an extensive background (or "conditioning") model [for details, see @oecd2013b]. These background variables provide additional information about a person's likely literacy and numeracy proficiency and helps improve precision of the PVs. The NEPS data also includes 10 PVs per respondent. However, because the background model only includes a minimal set of variables, we estimated a custom set of `r reading_neps %>% summarize(n_imp = max(.imp))` plausible values based on a more comprehensive background model via the *NEPSscaling* package [@scharl2020]. ^[\ This background model included age, age squared and age cubic, gender, educational qualification (ISCED levels), mother tongue, household size, federal state of Germany, gross monthly income, town size, number of books in the household cumulative employment duration across the lifespan, father's SES, meta-cognition in the same domain (i.e., math or reading), the competence levels in the other domain at both $T_1$ and $T_2$, fluid intelligence (reasoning ability), science competence, and ICT competence. For further details, see the R code for PV estimation for this paper in the first author's github repository at [blinded for review]. ] Although even a single PV would allow for unbiased estimates and most studies offer 10 PVs (as in PIAAC), a higher number of PVs improves precision and reduces standard errors.
In both datasets, we ran each of our analysis (described below) separately on each of the plausible values and aggregated the results according to Rubin's rules [@rubin1987]. 

## Analyses

### Measures of Change
Our first measure of change is based on the change (or difference) score between two measurement occasions, $\Delta{i, T_1, T_2} = y_{i,T_2} - y_{i,T_1}$. By aggregating the change score across all individuals in the sample or a subgroup, we obtain our estimate of interest, mean-level change in the population or subgroup in question:
$$\Delta{T_1, T_2} = \sum_{i=1}^{n} \frac{y_{i, T_2} - y_{i, T_2}}{n} $$ 
This is the most straightforward and widely used measure of change in an outcome in developmental research. Note that values of $\Delta{T_1,T_2}$ are in the raw metric of the tests (0-500 points in PIAAC; logits in NEPS). Therefore, to enhance comparability within and across studies, we report the standardized effect size Cohen's $d_{av}$, which expresses the average (mean-level) change in skills over in units of the standard deviation of the test. As is conventional [@lakens2013], we used the pooled standard deviation from both time points:
$$d_{av} = \frac{\Delta{T_1, T_2}}{\sigma_{pooled}} = \frac{\sum_{i=1}^{N} \frac{y_{i, T_2} - y_{i, T_2}}{N}}{0.5 \times (SD_{T_1} + SD_{T2})}$$.
Our second measure of change was the rank-order consistency [sometimes called "differential stability"; e.g., @schalke2013]. Its measure is the Pearson correlation between the tests at two time points,
$$r_{T_1,T_2} = \frac{\sum_{i=1}^{N} (y_{i,{T_1}} - \overline{y}_{T_1}) \times (y_{i,{T_2}} - \overline{y}_{T_2}) }{\sigma_{T_1}\sigma_{T_2}}$$, where $Y$ refers to the literacy or numeracy proficiency of an individual $i$ at time $T$. Rank-order consistency is a widely used standardized effect size measure that can be readily compared within and across studies.
The two measures of change offer complementary information [@deary2014]: Whereas $r_{T_1,T_2}$ refers to how stable individuals' relative standing in the skill distribution is across two measurement occasions (i.e., the stability of individual differences), $\Delta{T_1,T_2}$ refers to the how much individuals' skills change over time in absolute terms. Absolute change is largely independent of relative change: Even high rank-order consistency (e.g., $r_{T_1,T_2} > .90$) do not preclude the possibility of substantial mean-level change -- as long as skill gains or losses are very similar in size across individuals (i.e., $\Delta{T_1,T_2}$ has little variability).

### Subgroup analyses
Our analyses of variation in stability and change of skills across sociodemographic subgroups required that we split the age and education variables in a way that was (a) theoretically meaningful; (b) fine-grained enough to detect potential differences while (c) resulting in subgroups large enough to ensure stable estimates of change in both NEPS and PIAAC-L. Using these criteria, we chose to code age in four groups: young adults (18--34 years), prime-age adults (35--44 years), mid-aged adulthood (45--54 years) and older adults (55 years and older). We chose to code educational qualification in three groups according to the International Standard Classification of Education (ISCED 1997): lower or upper secondary ("low"; ISCED levels 0--3), post-secondary or tertiary vocational education ("intermediate"; ISCED levels 4 and 5B), and tertiary academic education ("high"; ISCED levels 5A and 6). For gender, we used a binary variable distinguishing men from women (PIAAC and NEPS did not yet include a diverse/third gender category).

# Results

## Descriptive statistics on skills, skill change, and cross-sectional age profiles

Before inspecting our two indices of focal interest--mean-level change and rank-order consistency--we inspected descriptive statistics on skills and skill change in the original raw metric of the tests (0-500 points in PIAAC; logits in NEPS). These are shown in Table\ \@ref(tab:table1) (using unweighted data) and supplementary Table\ \@ref(tab:tableS1) (using weighted data). Most important, these descriptive statistics show that the means (*M*) and standard deviations (*SD*) of both literacy and numeracy changed little between $T_1$ and $T_2$, although in NEPS the standard deviations of skills at $T_2$ were somewhat smaller than at $T_1$. Furthermore, whereas means of literacy and numeracy hardly changed over time, the *SD* of the change scores was indicative of considerable individual differences in change.

We also computed statistics on the cross-sectional age profiles of literacy and numeracy skills that, although not of primary interest to our present study, serve as informative points of comparison. First we plotted the cross-sectional age profiles of literacy and numeracy skills. These plots, shown in supplementary Figure\ \@ref(fig:figureS1), confirmed that literacy and numeracy in both PIAAC-L and NEPS conformed to the age trends know from earlier studies based on cross-sectional data [e.g., @desjardins2012; @paccagnella2016; @salthouse2019; @oecd2016]. Specifically, skills tended be lower among older respondents, with an inflection point around age 30 to 40 years.
Second, we estimated linear age effects on skills from the cross-sectional $T_1$ data, shown in supplementary Table\ \@ref(tab:tableS2). The resulting estimates of age differences in skills per year, study period, and decade served as a point of comparison for our longitudinal estimates of mean-level change. 

## Mean-level change in literacy and numeracy skills

(ref:figure1-caption) Mean-level change in literacy and numeracy skills across three years in PIAAC-L and six years in NEPS. 

```{r figure1, echo = FALSE, warning = FALSE, fig.cap = "(ref:figure1-caption)", fig.env = "sidewaysfigure", out.extra = ""}

knitr::include_graphics("deltas.pdf")

```
Figure\ \@ref(fig:figure1) presents results for our first measure of change: mean-level change in literacy (blue) and numeracy (red), expressed in Cohen's $d_{av}$, across three years (PIAAC-L, left panel) to six (NEPS, right panel) years of adulthood in the total population and in the subgroups.

In PIAAC-L, there was little evidence for mean-level change across three years in literacy in the total population. The same applied to numeracy. When looking at change separately in sociodemographic subgroups, few differences emerged. For literacy (but not numeracy), there were small gains among the most highly educated. Moreover,there were small gains in both literacy and numeracy in the youngest age group.
Likewise, in NEPS, there was little evidence for mean-level change across six years in literacy or numeracy in the total population. In contrast to PIAAC-L, the tendency of most effect sizes was towards losses, not gains, in literacy and numeracy. Subgroup differences were mostly minor: Apart from a small gender difference in numeracy change favoring women, highly-educated individuals experienced losses in literacy and, to a lesser extent, numeracy skills over the six-year period that were quite sizable, given the relatively short time period. Moreover, there was a tendency (although less pronounced than in PIAAC) for gains in the youngest and losses in the older age groups, although again the statistical uncertainty about these estimates was considerable.
However, as the standard deviations of the change scores in Tables\ \@ref(tab:table1) and \@ref(tab:tableS1) demonstrated, there was considerable heterogeneity behind the small or zero mean-level changes in Figure\ \@ref(fig:figure1). A closer look at the distribution of the change scores of literacy and numeracy, shown in supplementary Figure\ \@ref(fig:figureS2) confirmed this impression. In both studies, the distribution of the change scores of both literacy and numeracy were roughly symmetric around a near-zero mean. A considerable share of respondents experienced upward or downward skill change, and both occurred in roughly equal measure.

## Rank-order consistencies

(ref:figure2-caption) Rank-order consistencies of literacy and numeracy skills across three years in PIAAC-L and six years in NEPS. 

```{r figure2, echo = FALSE, warning = FALSE, fig.cap = "(ref:figure2-caption)", fig.env = "sidewaysfigure", out.extra = ""}

knitr::include_graphics("cors.pdf")

```
Figure\ \@ref(fig:figure2) shows the rank-order consistencies of literacy and numeracy in PIAAC-L (left panel) and NEPS (right panel). For an even more intuitive visualization, supplementary Figure\ \@ref(fig:figureS3) shows scatterplots of $T_1$ for the first five PVs of each skill and study.

In PIAAC-L, three-year rank-order consistencies for both literacy and numeracy were both above .80 . Variation across subgroups was very limited: For both skills, all correlations ranged between $`r cors %>% filter(grepl("piaac", target)) %>% summarise(min(rho)) %>% printnum()` \leq r_{T_1, T_2} \leq `r cors %>% filter(grepl("piaac", target)) %>% summarise(max(rho)) %>% printnum()`$. The correlations in different subgroups also had widely overlapping confidence intervals.

In NEPS, a slightly different picture emerged. For literacy, the six-year rank-order consistencies were lower than in PIAAC, and roughly as high as one would expect from the twice as long time interval between assessments (squaring the PIAAC-L rank order consistency gives $r = `r .83^2`$). (The lower rank-order consistency is also evident from the larger scatter of the points and the flatter regression line in Figure\ \@ref(fig:figureS3).) For numeracy, the rank-order consistencies in the total sample and all but one subgroups were higher ($`r cors %>% filter(target == "math_neps") %>% summarise(min(rho)) %>% printnum()` \leq r_{T_1, T_2} \leq `r cors %>% filter(target == "math_neps") %>% summarise(max(rho)) %>% printnum()`$) than those for literacy ($`r cors %>% filter(target == "reading_neps") %>% summarise(min(rho)) %>% printnum()` \leq r_{T_1, T_2} \leq `r cors %>% filter(target == "reading_neps") %>% summarise(max(rho)) %>% printnum()`$), and only minimally smaller than those in PIAAC-L. Within each skill, there was little variation across sociodemographic subgroups as in PIAAC, with confidence intervals overlapping widely. 

In sum, the rank-order consistencies -- indicative of "differential stability" -- further reinforced the view that a great deal of individual differences in change lie behind the small-to-zero mean-level changes. 

# Discussion
Are literacy and numeracy skills set like plaster after schooling age -- or do they continue to change throughout adulthood? If the latter, does change involve gains or losses, and how is it distributed in the population? The present paper used some of the best available data to answer these question: the German PIAAC-L and NEPS studies, offering complementary information about change in adults' literacy and numeracy skills across a three-year (PIAAC-L) to six-year (NEPS) period. 

Our analyses yielded three main insights. First, on average, literacy and numeracy skills change very little across a period of about half a decade covered by the two studies. There was a tendency toward gains in literacy but not numeracy in the total population in PIAAC-L and a tendency toward losses in both competences in NEPS. However, effect sizes for mean-level change hovered around zero for both skills and in both studies ($`r deltas %>% filter(grepl("total", grouping)) %>% select(dav) %>% min() %>% printnum()` \leq d \leq `r deltas %>% filter(grepl("total", grouping)) %>% select(dav) %>% max() %>% printnum()`$). 

Second, however, we found a considerable degree of individual differences behind these negligible mean-level changes in skills. To begin with, the change scores $\Delta{T_1, T_2}$ had considerable variation (especially in NEPS) and, as Figure\ \@ref(fig:figureS2) showed, were approximately symmetrically distributed around their near-zero mean, implying that gains and losses in literacy and numeracy over time were nearly equally prevalent. Further adding to this picture, the rank-order consistencies in the total population $r_{T_1, T_2}$ were high but not perfect across the three-year period in PIAAC ($`r cors %>% filter(grepl("piaac", target) & grepl("total", grouping)) %>% select(rho) %>% min() %>% printnum() ` \leq r_{T_1, T_2} \leq `r cors %>% filter(grepl("piaac", target) & grepl("total", grouping)) %>% select(rho) %>% max() %>% printnum()`$) and only moderate across the six-year period in NEPS ($`r cors %>% filter(grepl("neps", target) & grepl("total", grouping)) %>% select(rho) %>% min() %>% printnum() ` \leq r_{T_1, T_2} \leq `r cors %>% filter(grepl("neps", target) & grepl("total", grouping)) %>% select(rho) %>% max() %>% printnum()`$). This implies that individual differences in skills were not fully stable over time because some individuals experienced skill change. Literacy was significantly less rank-order consistent than numeracy in NEPS, yet this difference did not replicate in PIAAC where, if anything, literacy appeared slightly more consistent.

To put the rank-order consistencies of literacy and numeracy into perspective, several studies on cognitive development and aging found higher rank-order consistencies for measures of intelligence over longer periods of time. For example, @gow2012, in one of the few longitudinal studies on the stability of cognitive ability across the life span, found that general intelligence (*g*) measured with the Moray House Test No. 12 (MHT) at age 11 correlated at $r = .67$ ($r = .78$ after disattenuation) with intelligence at age 70 in 1,017 Scottish individuals from the Lothian Birth Cohort study. Other cohort studies from Scotland yielded comparable estimates but found rank-order consistencies to decline somewhat as the age of individuals at retest further increased into the eigth decade of life [@deary2014]. Similarly, @schalke2013 reported rank-order consistencies of $r = .85$ for *g* from age 12 to age 52, with only slightly lower estimates for more specific abilities (e.g., fluid reasoning, comprehension knowledge, visual processing) among 344 individuals from Luxembourg. One should bear in mind that the measures of intelligence in these studies are not directly comparable to the measures of literacy and numeracy in NEPS [recall that literacy and numeracy are best viewed as broad abilities on Stratum II in the CHC model of intelligence; @mcgrew2009]. Still, it is clear that the rank-order consistencies of literacy and numeracy over three to six years in PIAAC-L and NEPS are relatively low in comparison to that of general intelligence in these studies. 

Third, although there was considerable variation in skill change over time, few of these differences were systematic across sociodemographic subgroups. Most of the subgroup differences in mean-level change were small and, even more important, did not replicate across the two studies. In fact, some of the subgroup differences were in outright opposite direction in the two studies: The gender differences in mean-level change observed in NEPS were not present in PIAAC-L; the age differences in PIAAC-L roughly followed the well-known pattern from cross-sectional data (gains in younger adults, no change or losses in older adults) but not as clearly in NEPS; and the literacy gains of higher-educated individuals in PIAAC-L were starkly contrasted by losses in NEPS. The losses in literacy skills among the highly educated in NEPS was the strongest mean-level difference we observed ($d = `r deltas %>% select(dav) %>% min()`)$; change of about a fourth of a standard deviation is sizable, given the relatively short period of six years. The stronger declines among the highly educated observed in NEPS are in line with the cross-sectional age profiles of different educational groups reported by @paccagnella2016, which he labeled as "surprigsing" (p. 66). However, this finding may be indicative of bottom and/or ceiling effects, combined with regression to the mean, in the NEPS assessment compared to that in PIAAC. Consistent with this conjecture, additional analyses showed that the $T_1$ literacy and numeracy scores correlated with the change score at $`r cors_t1_delta %>% filter(study == "neps") %>% select(rho) %>% min()` \leq r \leq `r cors_t1_delta %>% filter(study == "neps") %>% select(rho) %>% max() %>% printnum()`$ but only at $`r cors_t1_delta %>% filter(study == "piaac") %>% select(rho) %>% min()` \leq r \leq `r cors_t1_delta %>% filter(study == "piaac") %>% select(rho) %>% max() %>% printnum()`$ in PIAAC-L. Likewise, recent studies investigating literacy skill change in NEPS separately by initial skill level [@wicht2021] showed that very low or very high $T_1$ literacy values regressed to the mean at $T_2$, and that higher-educated individuals in NEPS experience relative gains, not losses, in literacy, after controlling for initial skill levels in regression [@wicht2020]. 

Subgroup differences in rank-order consistencies were no more pronounced than those in mean-level stabilities. The only meaningful difference we found was the lower rank-order consistency of literacy (but not numeracy) in the oldest compared to the younger age groups in NEPS (but not PIAAC). Because of the limited sample size in these subgroups, the statistical uncertainty about these difference is high, however. Thus, we conclude that sociodemographic subgroups do not differ strongly in change and stability of literacy and numeracy skills. 

## Limitations and Directions for Future Research
The major advancement made by our study was to approach the question of change and stability in adults' literacy and numeracy skills through the lens of repeated-measures data *and* large-scale, non-selective samples, and to apply PV methodology [@wu2005; @vondavier2009] to account for measurement error in skills. This combination has so far been rare, if not completely absent from the literature, as previous evidence on age-related changes in literacy and numeracy skills hails either from large-scale cross-sectional or small-scale (and often highly selective) longitudinal studies [@paccagnella2016; @desjardins2012; @reder2009]. We are not aware of any comparably rich longitudinal data sources on adults' literacy and numeracy skills. Still, our study leaves several questions unanswered that future research should address. 

First, our data covered only two measurement occasions spanning time periods of three to six years. As our analyses show, even such relatively limited time periods allow for change in literacy and numeracy to occur. Nonetheless, six years represent a mere `r 6 / 80.99 *100`% of the current average life span of an individual born in Germany. Hence, future studies that follow individuals over longer time periods and assess literacy and numeracy at multiple time points would enable more nuanced insights into the temporal dynamics of change in literacy and numeracy (e.g., though growth curve modeling). Of note, it is not necessarily the case that rank-order consistencies are lower for longer time intervals, and it will be interesting how the stability of literacy and numeracy plays out in the long-run. So far, however, no truly large-scale, long-running panel studies measuring adults' literacy and numeracy skills in the sense of broad competencies ^[\ Note that there is handful of longitudinal studies on more narrow, circumscribed aspects of intelligence from research on aging [for discussions, see @deary2014]. However, these studies rarely measure broad competence domains such as literacy and numeracy, and some of them do not conform with modern psychometric standards.] at multiple time points are available, even though NEPS is ongoing and will continue to grow in this regard. 

Second, as noted, the assessments in PIAAC and NEPS were not high-stakes assessments. Although interviewers in both studies did their best to ensure that respondents took the tests seriously, the tests are more likely to represent "typical" as opposed to "maximum" performance. Whether typical or maximum performance in more relevant for life outcomes can be debated, yet is clear that high-stakes assessments might lead to different estimates of stability and change. In high-stakes assessment, performance tends to depend less on extraneous factors such as test motivation because most individuals are highly motivated in high-stakes assessments. Because it is almost impossible to implement repeated-measures high-stakes assessments, future studies should investigate whether extraneous factors such as test motivation influence estimates of change and stability in literacy an numeracy. 

Third, the differences between PIAAC-L and NEPS in the patterns of stability and change that we observed highlight that the assessments do produce slightly different estimates of change and stability. It will be important to understand how research designs and assessment approaches influence estimates of stability and change in adult skills. As noted, PIAAC-L and NEPS use similar but not identical assessment approaches [see @durda2020 for a discussion], and resulting scores are highly but not perfectly correlated [@carstensen2017]. One can speculate that the NEPS assessment -- which is designed to enable comparisons with the other NEPS cohorts, including children and youth [@weinert2011; @gehrer2013] -- is more susceptible to bottom and/or ceiling effects than that of PIAAC, which is tailored to adults. Moreover, both studies suffer from panel dropout, especially of those with lower education and skills [@martin2020]. Although we used weights to remedy the potential bias, future studies should investigate the extent to which selectivity may still bias results. 
Finally, it is interesting to note that in NEPS, the age differences suggested by the cross-sectional age profiles mostly exceeded these longitudinal mean-level changes. For example, literacy dropped by almost $-1 \ SD$ between age 40 and age 60 according to the cross-sectional age differences in $T_1$ numeracy (see Figure\ \@ref(fig:figureS1); the mean difference in numeracy skills between the youngest (24--34 years) and oldest (55+ years) age groups amounted to $`r age_diffs %>% filter(target == "math_neps") %>% pluck("young_old") %>% printnum()` \ SD$. As shown in Table\ \@ref(tab:tableS2), numeracy in NEPS changed at an average rate of $`r age_effects %>% filter(Change == "Per decade (SD)") %>% select("Numeracy (NEPS)")` \ SD$ per decade of life (`r age_effects %>% filter(Change == "Per period (SD)") %>% select("Numeracy (NEPS)")` *SD* for the six-year study period). These numbers are larger than the six-year mean-level changes estimated from the longitudinal data. The situation was similar for literacy in NEPS. By contrast, in PIAAC-L, the cross-sectional age differences were close to longitudinal mean-level change. There are some indications from prior research that repeated participation in cognitive assessments (retest effects) may bias stability estimates -- and that, for this reason, cross-sectional age profiles can sometimes capture age effects on cognitive skills more accurately [@salthouse2019]. The extent to which this applies in modern cognitive assessments in which only a subset of items are repeatedly administered (i.e., the "anchor items") is unclear. Further research is needed to pinpoint the reasons behind the divergences of the cross-sectional age profiles and longitudinal mean-level changes in NEPS.

## Conclusion
Taken together, our analyses in two longitudinal large-scale assessment surveys suggest that adults' literacy and numeracy skills are not set like plaster. Despite their substantial heritability [for a meta-analysis, for a meta-analysis on literacy, see @andreola2020], skills show lifelong plasticity. Whereas skills changed very little on average, the individual differences in change (i.e., the variation around the mean of the change scores) and the high but less-than perfect rank-order consistencies suggest that a substantial share of individuals do experience change in literacy and numeracy over time. This change comprised gains and losses in almost equal share, such that gains and losses canceled each other out when looking only at mean-level change. Thus, literacy and numeracy skills -- at least as measured in the PIAAC-L and NEPS assessments -- are not impervious to change even across relatively short time periods of three to six years. Our finding that literacy and numeracy are not impervious to change even over relatively short time periods of three to six years would seem to bolster recent efforts at using longitudinal data to identify factors that govern proficiency gains or losses over time -- which may provide targets for policy and interventions. Examples include skill use or practice engagement [e.g. @reder2009; @reder2020], continued education and training [@gauly2020; @gauly2019], or labor market participation [@wicht2020; @wicht2021], or basic skill programmes [e.g., @reder2009; @wolf2014]. Our findings can serve as a benchmark against which to compare future longitudinal findings on stability and change in literacy and numeracy.

\newpage
# Data availability statement
The data used in study are public and available upon registration from the NEPS and PIAAC/PIAAC-L websites, respectively.  

Data from the National Educational Panel Study (NEPS): Starting Cohort Adults (doi:10.5157/NEPS:SC6:11.0.0) are available from https://www.neps-data.de/Data-Center/Data-and-Documentation/Start-Cohort-Adults 
Data from PIAAC-L are available from https://www.gesis.org/en/piaac/rdc/data/piaac-longitudinal.

For full transparency, the complete R code for the analyses and the R markdown file for the manuscript [created with the R package papaja; @aust2020] are available from the first author's github repository at [blinded for review].

\newpage
# Ethics statement
This paper uses secondary data. These data were collected in accordance with common ethical guidelines: All individuals involved in PIAAC-L and NEPS gave their written consent to participate in this study after they had been informed about the purposes of the project, the content covered by the questionnaire, and their right to revoke consent at any time. All data were processed in accordance with the pertinent data protection rules in order to ensure the anonymity of participants. For more information, please see the project websites of NEPS and PIAAC-L.

\newpage
# Disclosure statement
The authors report no potential conflict of interest.

\newpage
# Funding statement
This paper was supported by a grant by the German Research Foundation (DFG), "Stability and Change in Adult Competencies" (Grant No. LE 4001/1-1); and a grant by the Germany Federal Ministry of Education and Research (BMBF), "Risk and protective factors for the development of low literacy and numeracy in German adults" (Grant No. W143700A). 

\newpage
# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup


```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('appendix.Rmd')
```

