---
title             : | 
  Stability and Change in Adults' Literacy and Numeracy Skills: Evidence From two Large-Scale Panel Studies
shorttitle        : "Stability and Change in Adults' Literacy and Numeracy"
author:
  - name          : "Clemens M. Lechner"
    affiliation   : "1"
    corresponding : yes    
    address       : "B2, 1, 68159 Mannheim, Germany"
    email         : "clemens.lechner@gesis.org"
    role:          
      - Funding acquisition
      - Supervision
      - Conceptualization
      - Data curation
      - Writing - Original Draft 
      - Writing - Review & Editing
      - Methodology 
  - name          : "Britta Gauly"
    affiliation   : "1"
    role:
      - Data curation
      - Writing - Review & Editing
  - name          : "Ai Miyamoto"
    affiliation   : "2"
    role: 
      - Writing - Review & Editing
  - name          : "Alexandra Wicht"
    affiliation   : "3"
    role:
      - Data curation
      - Formal analysis
      - Writing - Review & Editing
affiliation:
  - id            : "1"
    institution   : "GESIS -- Leibniz Institute for the Social Sciences, Mannheim, Germany"
  - id            : "2"
    institution   : "University of Freiburg, Germany"
  - id            : "3"
    institution   : "University of Siegen, Germany"
authornote: |
  The authors thank William Thorn (OECD) for helpful comments on an earlier version of this investigation.
abstract: |
 
 
 We investigated individual differences in the development of literacy (reading competence) and numeracy (mathematical competence) during adulthood using data from two large-scale multi-wave studies: the PIAAC-longitudinal Study (PIAAC-L, *N* = 2,779) and the National Educational Panel Study (NEPS; *N* = 3,140). We examined mean-level change ($\Delta{T_1,T_2}$) and rank-order consistencies ($r_{T_1,T_2}$) of literacy and numeracy over three to six years in the total adult population and in sociodemographic subgroups defined by age, gender, and education. To account for measurement error, we employed plausible values (PV) methodology. Results revealed that literacy and numeracy are highly but not perfectly rank-order stable ($.66 \leq r \leq .83$). Mean-level change was negligible for both skills and studies, but there were considerable individual differences in change. Apart from moderate mean-level gains in literacy in young adults (18--29 years), there were few differences in mean-level or rank-order change by age group, gender, and education, and the few subgroup difference we found did not replicate across studies. Individual difference in change dominate the picture. The same applied to rank-order stabilities. Our findings suggest that adults' literacy and numeracy are malleable throughout adulthood and can change even over a three- to six-year period. They can serve as a benchmark against which to compare future longitudinal findings.
 
keywords          : "literacy, numeracy, intelligence, development, adulthood, longitudinal"
wordcount         : ""
bibliography      : ["r-references.bib"]
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
numbersections     : yes
csl               : "apa.csl"
#documentclass     : "apa6"
documentclass     : "apa7"
classoption       : "man"
output            : papaja::apa6_word
appendix:  "appendix.Rmd"
header-includes   :
 # - \usepackage{rotating}
 # - \DeclareDelayedFloatFlavor{sidewaysfigure}{figure}
# - \usepackage[default]{sourcesanspro}
  - \usepackage[T1]{fontenc}
  - \usepackage{caption}
  - \usepackage{makecell}
  - \usepackage{multirow}
---

```{r setup, include = FALSE}
# Load required packages
#options(Encoding="UTF-8")

dirs <- list(
  results = "./02_results"
  
)

library(tidyverse)
library(glue)
library(sjlabelled)
library(kableExtra)
library(extrafont) 
library(papaja)
library(mice)
library(miceadds)
library(mitools)
library(srvyr)
library(cowplot)
library(survey)

# BibLateX Reference file
r_refs("r-references.bib")

# Load raw data and results
map(str_c(dirs$results, "/", 
            c("data_neps.Rda",
              "data_piaac.Rda",
              "results.Rda",
              "reports.Rda")),
    load, .GlobalEnv)

# Survey package options (for Appendix)
options(survey.adjust.domain.lonely=TRUE)
options(survey.lonely.psu="adjust")
options(survey.multicore = TRUE)
use_weight <- ~weight # Use cross-sectional weight

```

# Introduction

In today's knowledge-based and technology-rich societies, literacy (i.e., reading competence, or the ability to understand, use, and interpret written text) and numeracy (i.e., mathematical competence, or the ability to access, use, and interpret mathematical information) are quintessential skills[^1] for the welfare and well-being of individuals and societies. Individual differences in literacy and numeracy are linked to important outcomes such as income, health, and social participation as well as and macro-level societal outcomes such as economic growth [e.g., @oecd2013; @oecd2016; @hanushek2015; @hanushek2015a].

[^1]: Whereas PIAAC and the international large-scale assessment community mostly prefers the term literacy and numeracy skills [@oecd2012; @cunha2007], NEPS refers to them as reading competence" and mathematical competence [@weinert2011]. Both terms emphasize that literacy and numeracy are broad, multi-faceted abilities that can be acquired through education and experience. Moreover, both PIAAC and NEPS operationalize these constructs from a functional perspective that focuses on the capacity to carry out everyday tasks. Research on individual differences in psychology, by contrast, has traditionally preferred the terms "intelligence" or "cognitive ability" to denote different types of competences or skills, including those that are sensitive to education and experience [e.g., @cattell1971; @mcgrew2009].Yet others use the more neutral term "proficiency" [e.g., @reder2020]. Although these terms all have slightly different connotations, they reflect different research traditions and communities, rather than denoting fundamentally different concepts. For consistency, we used the term "skills" throughout this article.

Demographic aging in most industrialized societies and rapid technological advances imply that the adult workforce will be increasingly required to update their skills throughout the life course, often well into the sixth decade of life. Literacy and numeracy are key prerequisites to acquiring more specific knowledge and skills (e.g., reading a machine's manual or programming a computer), and hence for lifelong learning at large. Because not all individuals attain sufficient levels of literacy during schooling age [@durda2020; @wicht2021; @wolf2014], the key question is whether literacy and numeracy skills can still change during adulthood; and if so, whether such change would involve only losses -- or proficiency gains at least in some adults? Moreover, how is skill change distributed in the adult population and in subgroups such as age groups, genders, and educational strata? Potential losses in skills over time (in at least some segments of the adult population) might explain why many adult surveys such as PIAAC or LEO find substantial shares of adults with low competences despite these adults having completed 9 or more years of schooling and holding vocational qualifications [e.g., @grotluschen2020; @grotluschen2016; @durda2020].

These are not only interesting research questions in their own right, they may also be relevant to policymakers and practitioners who are interested in promoting lifelong learning [e.g., @wolf2014]. For example, if literacy and numeracy were impervious to change during adulthood, investments in basic skills or workplace learning programs aimed at fostering literacy and numeracy during adulthood might have a low return on investment, and childhood may be a more promising life stage for policies and interventions to focus on [e.g., @cunha2007]. Moreover, literacy and numeracy may change over time, but gains and losses may be unevenly distributed among (different groups of) individuals. Findings regarding the differential stability of literacy and numeracy in different groups may help identify segments of the population who are at a heightened risk of experiencing skill loss over time (e.g., older adults or the lower-educated). These population segments may be in need of support from policymakers and practitioners (such as by providing them access to basic skill programs or workplace-based skill programs). Thus, a better understanding of stability and change of literacy and numeracy skills in adulthood may aid the development of targeted policies and interventions specifically for these groups in the future.

## Previous Research

Given these policy relevance of literacy and numeracy development during adulthood, what answers can extant research provide us?

### Theoretical perspectives on skill development in adulthood

Prominent theoretical perspectives on skill development in adulthood agree that skills are characterized by lifelong plasticity, yet they differ in their predictions as to when skill change may occur and for whom, as well as whether it involves gains or losses.

In his seminal work on cognitive ageing, @cattell1971 hypothesized differential age trajectories for fluid intelligence (*Gf*, or the ability to process information, perceive and discriminate object relations, and resolve novel problems) and crystallized intelligence (*Gc*, the totality of knowledge and skills acquired over time). According to his still influential theory, *Gf* -- which is largely innate and depends on biological functioning -- starts to decline after the second decade of life, whereas *Gc* -- which results from the investment of *Gf* in different subject areas and is influenced by education, experience, and culture -- continues to increase well into adulthood and only declines in very old age. Later, research on cognitive aging built on this idea. Perhaps most prominently, Baltes's [-@baltes1993] distinguished between cognitive mechanics (comparable to *Gf* ) and cognitive pragmatics (comparable to *Gc*). He, too assumed that cognitive pragmatics reflect the impact of culture and learning and would therefore remain stable or even increase until old age. Literacy and numeracy as measured in the two large-scale assessment surveys used in this paper (for details, see Method) are conceptualized as skills that are acquired through education and practice in different contexts. Although these skills require the operation of *Gf* or cognitive mechanics (e.g., identifying and processing visual stimuli while reading), as acquired skills they correspond closely to the conception of *Gc* or cognitive pragmatics in Cattell's and Baltes's work. The definitions and operationalizations of literacy and numeracy in these surveys also correspond closely to reading and writing ability (*Grw*) [^2] and quantitative knowledge (*Gq*), respectively, in the updated Cattell-Horn-Carroll (CHC) model of intelligence, one of the most prominent models of the structure of intelligence [@mcgrew2009]. The CHC model, too, classifies *Grw* and *Gq* as *acquired* abilities. Hence, based on prominent theories of intelligence, it can be expected that literacy and numeracy skills remain largely stable or even increase across adulthood -- until perhaps very old age, when physical aging and limitations lead to declines even in acquired skills.

[^2]: Note that the functional literacy tests in PIAAC-L and especially NEPS focus on *text comprehension* but not writing (text production) [@gehrer2013]. Hence literacy in these studies measures a subset of *Grw* as conceived in the CHC model of intelligence, which comprises both reading and writing ability.

Another prominent perspective from research on adult learning is practice engagement theory [PET; @reder1994]. PET originates from literacy research but its ideas apply equally to other skill was recently extended to numeracy [@reder2020]. It posits that individuals' literacy proficiencies develop as a by-product of their engagement in everyday reading and writing practices and, reciprocally, that literacy proficiencies affect levels of engagement in reading and writing practices. PET would predict that adults can experience both gains and losses in literacy and numeracy skills over time -- depending on the extent to which they engage in attendant practices in their job, leisure, or other contexts. This central tenet of PET is compatible with Cattell's and Baltes's perspectives but extends these perspective by highlighting individual differences in skill development over time.

A third perspective that hails from life course sociology is the Matthew effect hypothesis, also known as the cumulative (dis-)advantage hypothesis [@dannefer1987]. Applied to skill development over the life course [see @blossfeld2011a], this perspective states that differences in initial skill levels become magnified over the life course. Through the joint effects of selection and socialization, individuals with higher initial skill levels will continue to gain in skills compared to those with initially lower skills. For example, those with higher skills as children will, on average, have higher educational attainment than those with lower skills, which in turn channels them into more complex jobs that foster the further development of skills. Like PET, the Matthew effect hypothesis emphasizes individual differences in skills and the relevance of social context in governing skill development over time.

Together, these theories highlight the lifelong plasticity of skills, age-dependency of skill development, differential development of fluid-type and crystallized-type (i.e., acquired) skills. Moreover, they emphasize that both gains and losses in skills can occur and highlight individual differences in skill development as a fundamental principle.

### Previous Evidence on Age Differences in Literacy and Numeracy

Existing evidence on age differences in literacy and numeracy generally support the theoretical perspectives outline above [for comprehensive reviews, see @desjardins2012; @paccagnella2016; also see @deary2014]. Three key insights offered by current evidence are the following : (1) Lifelong plasticity of skills, (2) life stage dependency of change, and (3) individual and subgroup differences in skill change.

First, literacy and numeracy appear to continue to develop across adulthood. Despite the fact that literacy (and likely also numeracy skills) have a genetic component [i.e., are heritable; @andreola2020], both cross-sectional and the few available longitudinal studies suggest that these skills are not "set like plaster" after childhood but continue to change across the lifespan [@desjardins2012; @paccagnella2016; @oecd2016].

Second, although skill change during adulthood may involve both gains and losses, gains and losses typically occur at different ages. As studies using data from international large-scale comparisons such as the Programme for the International Assessment of Adult Competencies (PIAAC) show, the cross-sectional age profile of literacy and numeracy follows an inverted U-shape: On average, literacy and numeracy skills continue to increase throughout the second decade of life, peak at around an age of 30 years, and gradually decline thereafter [e.g., @gabrielsen2014; @paccagnella2016; @podolskiy2014; @oecd2016]. The resulting age differences in skills are substantial: On average across participating countries, older adults (aged 55-65 years) score about 30 scale points lower on the PIAAC literacy scale (the equivalent of $0.8\ SD$) than young adults aged 25-34 years [@paccagnella2016].

Interestingly, the age trajectories of literacy and numeracy in the above studies neither follow Cattell's ideal-typical paths of *Gc* nor *Gf* but are best described as a mixture of both. The reason might be that although literacy and numeracy constitute acquired skills that are sensitive to education and experience (like Cattell's *Gc* and Baltes's cognitive pragmatics), they also depend on *Gf* , especially if tests are speeded (for details, see Measures). For comparison, longitudinal research on cognitive aging focusing on basic cognitive abilities found approximately linear declines in cognitive abilities (e.g., processing speed, reasoning, memory) from early adulthood onward that accelerated in old adulthood [@salthouse2019] -- with the exception of increases with age in vocabulary, which is closer to Cattell's conception of *Gc* than to *Gf* and an essential part of literacy skills.

Third, beyond these average age trends, there are additional individual and group differences in skill change. For example, using cross-sectional PIAAC 2012 data, @paccagnella2016 compared age differences in literacy and numeracy skills among adults with different levels of educational qualification (i.e., primary, secondary, and territory or above) and found that those with the highest educational qualification experienced slightly larger skill loss during adulthood than those with lower educational qualification By contrast, a recent longitudinal study in German adults found that higher education had a protective effect against declines in literacy over time after controlling for initial level of literacy skills [@wicht2020]. In addition, while some studies suggest a tendency of female adults having relative strength in literacy and of male adults having relative strength in numeracy across the life course [e.g, @houtkoop1998; @satherley2008], other studies show only a small to non-existent gender differences in adults' literacy skills [@solheim2018]. It remains unclear whether this translates to differential change over time as other studies did but not find any pronounced gender differences in cross-sectional PIAAC skill profiles [@oecd2016] and in skill change over time [@reder2009; @wicht2020]. Another longitudinal study found women to start off at higher literacy levels but experience smaller proficiency gains over time compared to men [@wolf2014].

Although these prior studies have greatly advanced our knowledge about skill development in adulthood, they cannot conclusively answer the questions posed at the outset. This is because these studies are overwhelmingly based on large-scale cross-sectional surveys such as PIAAC and its predecessors (i.e., the International Adult Literacy Survey in 1994 and 1998 or the Adult Literacy and Life Skills Survey in 2003, 2006, and 2008); or on small-scale longitudinal studies based on selective samples such as the longitudinal study of adult learners (LSAL) that focuses on high-school dropouts in the US [@reder2009] or adults in basic skills programs in the UK [e.g., @wolf2014]. Cross-sectional studies can be advantageous in that they do not suffer from selective attrition of low-skilled or lower-educated individuals, as most longitudinal assessment surveys do [@martin2020]. Moreover, they are unbiased from retest artifacts which plague some longitudinal surveys, especially ones that apply designs in which the exact same items are administered repeatedly [@salthouse2019]. At the same time, cross-sectional studies of age differences are limited in that they are unable to disentangle age-related changes from cohort effects. That is, they are unable to ascertain whether the putative age differences are due to age-related changes or stem from preexisting differences in skills during childhood. Small-scale longitudinal studies based on selective samples, on the other hand, are limited in that their findings may not generalize to the population as a whole. Moreover, by their very nature, these studies cover some subgroups (e.g., high-school dropouts) in a certain life stage (e.g., young adulthood) but not others (e.g., the highly educated) and life stages (e.g., old age) that may be of equal interest to policymakers and practitioners. Also, compared to literacy, the life-span development numeracy has received much less attention by prior research, despite arguments in the literature that numeracy skills are gaining in importance on today's labor markets [e.g., @gal2020; @gauly2020]. In order to overcome the limitations of cross-sectional and small-scale longitudinal designs, repeated measures of literacy and numeracy skills are needed. Such data have long been in short supply. Until very recently, there were simply no data sources available internationally that combined the following desirable features that would allow for complete and robust answers to questions surrounding age-related changes in skills during adulthood: A large and non-selective sample; objective, and high-quality skill assessments; and a repeated measures design.

## The Present Research

In the present study, we leverage the unique [^3] analytic potential of two recent German large-scale assessment surveys that do meet the above criteria: PIAAC-longitudinal (PIAAC-L), a follow-up to the 2012 Programme for the International Assessment of Adult Competencies (PIAAC) study in Germany; and Starting Cohort 6 from the National Educational Panel Study (NEPS). Both surveys offer repeated -- and largely comparable -- measures of adults' literacy and numeracy spaced three (PIAAC-L) to six years (NEPS) apart. Combining these data offers us the opportunity to analyze changes in literacy and numeracy skills overtime during adulthood, allowing us to present what appear to be the most comprehensive descriptive analyses of age-related changes of literacy and numeracy skills during adulthood to date based on a repeated-measures design.\
With these data, we seek to answer two guiding questions about the stability and change of adults' literacy and numeracy skills. First, to what extent do these skills change across the three- to six-year periods covered by PIAAC-L and NEPS? Second, does the extent of age-related change differ across major sociodemographic groups?\
We approach these questions from two perspectives on age-related change enabled by the repeated-measures designs [e.g., @deary2014]: absolute change and rank-order change (for details, see Method). We analyze skill change both for the total populations and in major sociodemographic subgroups defined by age, gender, and educational qualification. The subgroup analyses allow us to identify segments of the population in which change may be more pronounced than in others. Of particular interest to our study is the question whether the age differences in skills and subgroup differences therein that we obtain through our repeated-measures designs parallel, or differ from, estimates of age differences obtained in cross-sectional designs [e.g., those reported by @paccagnella2016; @deary2014]?


[^3] We know of now surveys that offer comparable data as PIAAC-L and NEPS, that is (a) repeated-measures data on adults' literacy and numeracy skills in (b) large random samples. Although there have been longitudinal add-ons to PIAAC 2012 in several other countries (Canada, Italy, and Poland), none of these studies comprised a follow-up assessment of literacy and numeracy.



# Method and Materials

## Data and Samples

### PIAAC/PIAAC-L

Our first data source is the German PIAAC/PIAAC-L study [version 3-0-0; @gesis2017]. PIAAC was conducted in 2012 and provides internationally comparable data on the skills of the working-age population (16-65 years) residing in private households in large number of (mainly OECD) countries [@oecd2013a]. In Germany, a registry-based sampling design was implemented, in which respondents were randomly sampled from local population registers in randomly selected German municipalities. A total of 5,465 interviews in respondents' homes were achieved. At the end of the PIAAC interview, all German PIAAC respondents were asked whether they were willing to be re-contacted for a follow-up study (i.e., PIAAC-L) in the future. Of the 5,465 respondents who had completed the PIAAC 2012 assessment (henceforth "$T_1$"), 3,758 consented to being re-interviewed. A total of 3,263 participated in the second wave of PIAAC-L 2015 in which literacy and numeracy were re-assessed (henceforth "$T_2$"). After excluding minors (younger than 18 years) and non-native speakers of German, our longitudinal sample contains the `r (nrow(reading_piaac) / max(reading_piaac$.imp)) %>% printnum(., digits = 0)` adults aged 18--65 at $T_1$ for who took the test at both measurement occasions.

Non-response analyses predicting non-participation at $T_2$ from the $T_1$ variables considered in our study showed that individuals with lower literacy and numeracy skills, low educational attainment and individuals from the youngest age group were likely to drop out of the panel. This indicates that the retest-sample is slightly skewed towards more highly educated and high-skilled respondents. For all longitudinal analyses, we therefore used weights that correct for selective non-response. Specifically, we multiplied the total population weight of PIAAC 2012 with a factor that accounts for non-response in PIAAC-L 2015.

As in other previous research using PIAAC-L [@gauly2019], we excluded respondents whose first language was not German (around 10% in PIAAC-L). This is because for these respondents, the literacy tests measure not just literacy but also foreign or second (German) language proficiency, which may bias the change over time they experience. Because the numeracy tests in PIAAC-L and NEPS are heavily text-based (and not "pure math"), the same applies to numeracy. Table\ \@ref(tab:table1) (right column) shows the sociodemographic characteristics and skill profile of the (unweighted) PIAAC-L sample. For more details on the samples, procedures and weighting, see the technical reports to PIAAC 2012 Germany and PIAAC-L [@zabal2014; @zabal2017].

### NEPS

Our second data source is Starting Cohort 6 (Adults) of NEPS [version 11.0.0; @artelt2020]. NEPS is an ongoing large-scale, multi-cohort, longitudinal survey on educational trajectories in Germany [for a detailed description including sampling procedures, see, @blossfeld2011]. Starting Cohort 6 comprised a sample of initially 11,649 adults aged 22--65 years who were interviewed in up to nine annual waves so far. Literacy and numeracy skills were assessed twice, first in 2010/2011 (henceforth "$T_1$") and again six years later in 2016/2017 ("$T_2$"). After excluding non-native speakers, our longitudinal sample comprised `r printnum((nrow(reading_neps) / max(reading_neps$.imp)), digits = 0)` respondents who took the literacy test at both occasions and `r printnum((nrow(math_neps) / max(math_neps$.imp)), digits = 0)` respondents for numeracy.

As in PIAAC-L, non-response analyses indicated that individuals with low skills, low education, and individuals from the youngest age group were more likely to drop out at $T_2$. Akin to PIAAC-L, we used longitudinal weights adjusted for wave-specific non-response. These weights (NEPS variable w_t456789_std) for individuals who continuously participated from$T_1$ (wave 3) until $T_2$ (wave 9) are computed from the longitudinal weight of the previous wave, the probability of being part of the used sample, and the likelihood of participating at $T_2$ [for details on non-response and the weighting procedure in NEPS, see @hammon2016; @zinn2018]. As in PIAAC-L, we excluded respondents whose first language was not German (aroung 4% of respondents.) Table 1 (right column) shows the sociodemographic characteristics and skill profile of the (unweighted) NEPS sample.

(ref:table1-caption) Sociodemographic characteristics of the two samples and skill profile of the two samples (unweighted sample statistics).

```{r table1, echo = FALSE}

# This relies on the tables created in 04_reporting.R

 apa_table(
    rbind(
    data_descriptives, data_skills
    ),
    col_spanners = list("\\makecell[c]{PIAAC-L\\\\(\\emph{N} = 2,779)}" = c(2,4),                       "\\makecell[c]{NEPS\\\\(\\emph{N} = 3,140)}"  = c(5,7)),
      stub_indents = list("Age" =  c(1:5),
                        "Education" = c(7:9),
                        "Literacy" = c(10:12),
                        "Numeracy" = c(13:15)),
    escape = F,
    align = "lcccccc",
    caption = "(ref:table1-caption)",
   font_size = "small",
    note = glue("The socio-demographic characteristics in NEPS are for the literacy sample; the NEPS numeracy sample was slightly smaller (N = {printnum((nrow(math_neps) / max(math_neps$.imp)), digits = 0)}) but had virtually the same sociodemographic profile. For literacy and numeracy skills, cell values represent pooled estimates across the 10 (30) PVs in PIAAC-L (NEPS), respectively. " )
  )


```

## Measures of Literacy and Numeracy

PIAAC/PIAAC-L and NEPS assessed literacy and numeracy in comparable ways [for a detailed comparison of the literacy assessments in PIAAC and NEPS, see @durda2020]. Both conceptualized literacy and numeracy from a functional perspective, with assessment items reflecting problems and tasks encountered in everyday life [@gehrer2013].

Despite some differences in the specifics of the assessment approaches (for details, see below), evidence from a linking study suggests high convergence between the PIAAC and NEPS tests [@carstensen2017]. As part of this linking study, a subset of participants of Wave 2 of PIAAC-L (2015) received different test versions, some of which contained items from both PIAAC and NEPS. Joint item response theory (IRT) scaling of the tests showed that the PIAAC literacy test correlates at $r = .87$ with the NEPS literacy test, and the PIAAC numeracy test correlates at $r = .90$ with the NEPS numeracy test. The correlations between the literacy and numeracy tests within each study were also very similar: PIAAC literacy correlated with PIAAC numeracy at $r = .87$, and NEPS literacy correlated with NEPS numeracy at $r = .87$. This pattern of correlations suggests that the tests measure largely the same latent constructs.

It is important to note that the assessments in both surveys were low-stakes assessment. Thus, the test scores are likely to reflect typical, rather than maximum performance, even though interviewers took steps to ensure that respondents took the test situation seriously and invested sincere efforts to resolve the items.

### PIAAC assessment approach

In PIAAC and PIAAC-L, literacy and numeracy skills were assessed through comprehensive, extensively validated (including cross-nationally) tests. Test tasks were devised by an international commission of eminent scholars [@expert2009a; @expert2009b]. The tasks were designed to reflect tasks relevant to everyday life, which respondents were typically highly motivated to solve. Respondents either took a computer-based assessment (86.2% in our analysis sample in 2012; 88.1% in 2015) or -- in case they were found to be unfit for the computer-based or refused to take it -- a paper-pencil version (13.8% in 2012 and 11.8% in 2015). Both assessments were scaled such that they are directly comparable [see @oecd2013b]. Depending on whether respondents took the computer-based assessment or paper-based assessment, they received different testlets that comprised different subsets of the test tasks. The computer-based assessment was a multistage adaptive testing design that consisted of different literacy and numeracy testlets (each comprising 20 tasks) to which respondents were assigned. The paper-based assessment comprised 20 fixed items for literacy and for numeracy. Interviewers were thoroughly trained for the assessment, they were present while respondents took the tests, and monitored the process. There was no time limit (i.e., tests were not speeded). On average, respondents took about 50 minutes to complete the assessment. @oecd2012 and @oecd2013 provide further information on the PIAAC assessment frameworks.

### NEPS assessment approach

In NEPS, literacy (or, in NEPS terminology, "reading competence") and numeracy ("mathematical competence") were assessed through a paper-pencil-based assessment at $T_1$ and a multistage adaptive computer-based assessment at $T_2$ Much akin to PIAAC, the tests were designed to measure skills needed in everyday life. Different from PIAAC, the tests were speeded. The $T_1$ literacy test comprised 32 items, which respondents were asked to complete in 28 minutes. The $T_1$ numeracy test comprised 22 items for which another 28 minutes were available. Respondents were randomly assigned different booklets, and not all respondents took both tests; however, all respondents received all items from a given booklet. At $T_2$, by a multistage design was employed, with respondents receiving one of two booklets of varying difficulty levels depending on their previous performance at $T_1$ (literacy) or their performance in an initial block of tasks at $T_2$ (numeracy). The $T_2$ literacy booklets comprised 27 or 26 items and the numeracy booklet comprised 21 items. For further information on the NEPS assessment frameworks, see @gehrer2013 for literacy and @neumann2013 for numeracy.

### Plausible values (PVs)

Literacy and numeracy skills are latent variables that cannot be observed directly but only inferred from individuals' responses to a series of test items. Point estimates of individual abilities (i.e., "test scores") such as number-right scores or weighted likelihood estimates [WLE; @warm1989], therefore, always contain measurement error. Measurement error can bias both rank-order consistencies (which are typically attenuated) and mean-level change (which can be over- or underestimated, depending on the method used and the extremity of an individual's score). To avoid such biases, we used plausible value (PV) methodology in both studies. PV methodology is currently the gold standard in international large-scale assessments. Instead of estimating a single test score per individual, PV appropriately accounts for the uncertainty about each individual's true skill by drawing multiple, equally "plausible" values of each individual's skills based on a model which includes responses to test items, as well as a large set of background variables (e.g., age, education, employment, motivation, reading practice, and many more). Conceptually, the different plausible values are multiple imputations of the missing skill score for each person and can be analyzed by standard multiple imputation (MI) methodology. The crucial advantage of PV for our present intent is that they allow for unbiased estimates of population quantities (including our two key measures of change, $\Delta{T_1,T_2}$ and $r_{T_1,T_2}$) that are corrected for measurement error [for further details on PV methodology, see @wu2005; @vondavier2009; for a non-technical introduction to the usage and advantages of PVs over more traditional methods, see @lechner2021].

The PIAAC/PIAAC-L data include 10 PVs per respondent, skill domain and measurement occasion based on item response theory models (IRT) based on an extensive background (or "conditioning") model [for details, see @oecd2013b]. These background variables provide additional information about a person's likely literacy and numeracy proficiency and helps improve precision of the PVs. The NEPS data also include 10 pre-computed PVs per respondent. However, because the background model only includes a minimal set of variables, we estimated a custom set of `r reading_neps %>% summarize(n_imp = max(.imp))` plausible values based on a more comprehensive background model via the *NEPSscaling* package [@scharl2020]. [^4]

Although even a single PV would allow for unbiased estimates and most studies offer 10 PVs (as in PIAAC), a higher number of PVs improves precision and reduces standard errors. In both datasets, we ran each of our analyses (described below) separately on each of the plausible values and aggregated the results according to Rubin's rules [@rubin1987].

[^4]: This background model included age, age squared, gender, migrant status (1st and 2nd generation), household size, federal state of Germany, town size, cumulative employment duration across the lifespan, father's SES, years of education, educational qualification (ISCED levels), gross monthly income, socio-economic status (ISEI), number of books in the household, amount of reading during leisure, reasoning ability (Gf), processing speed, meta-cognition in the same domain (i.e., math or reading), the competence levels in the other domain at both $T_1$ and $T_2$, science competence, and ICT competence. For further details, see the R code for PV estimation for this paper in the first author's github repository at [blinded for review].

## Analyses

### Measures of Change

Our first measure of change is based on the change (or difference) score between two measurement occasions, $\Delta{i, T_1, T_2} = y_{i,T_2} - y_{i,T_1}$. By aggregating the change score across all individuals in the sample or a subgroup, we obtain our estimate of interest, mean-level change in the population or subgroup in question: $$\Delta{T_1, T_2} = \sum_{i=1}^{n} \frac{y_{i, T_2} - y_{i, T_2}}{n} $$ This is the most straightforward and widely used measure of change in an outcome in developmental research. Note that values of $\Delta{T_1,T_2}$ are in the raw metric of the tests (0-500 points in PIAAC; logits in NEPS). Therefore, to enhance comparability within and across studies, we report the standardized effect size Cohen's $d_{av}$, which expresses the average (mean-level) change in skills over in units of the standard deviation of the test. As is conventional [@lakens2013], we used the pooled standard deviation from both time points: $$d_{av} = \frac{\Delta{T_1, T_2}}{\sigma_{pooled}} = \frac{\sum_{i=1}^{N} \frac{y_{i, T_2} - y_{i, T_2}}{N}}{0.5 \times (SD_{T_1} + SD_{T2})}$$ Our second measure of change was the rank-order consistency [sometimes called "differential stability"; e.g., @schalke2013]. Its measure is the Pearson correlation between the tests at two time points, $$r_{T_1,T_2} = \frac{\sum_{i=1}^{N} (y_{i,{T_1}} - \overline{y}_{T_1}) \times (y_{i,{T_2}} - \overline{y}_{T_2}) }{\sigma_{T_1}\sigma_{T_2}}$$ Here, $Y$ refers to the literacy or numeracy proficiency of an individual $i$ at time $T$. Rank-order consistency is a widely used standardized effect size measure that can be readily compared within and across studies.

The two measures of change offer complementary information [@deary2014]: Whereas mean-level change refers to the how much individuals' skills change over time in absolute terms, rank-order consistency refers to how stable individual differences in skills are across two measurement occasions (i.e., relative change). A perfect rank-order consistency of $r_{T_1,T_2} = 1$ would imply that the skill ranking of individuals stays exactly the same over time. Also note that mean-level change is largely independent of rank-order change: Even high rank-order consistency (e.g., $r_{T_1,T_2} > .90$) do not preclude the possibility of substantial mean-level change in the sample -- as long as skill gains or losses are very similar in size across individuals (i.e., $\Delta{T_1,T_2}$ has little variability).

In computing both measures of change (and all additional analyses), we used longitudinal weights supplied by PIAAC-L and NEPS, respectively, that align the distribution of socio-demographic characteristics with the German Microcensus and correct for non-response. For weighting, we used the survey package [@lumley2020], which allowed us to account for the primary samling units (PSU), strata, and weights.

### Subgroup analyses

Our analyses of variation in stability and change of skills across sociodemographic subgroups required that we split the age and education variables in a way that was (a) theoretically meaningful; (b) fine-grained enough to detect potential differences while (c) resulting in subgroups large enough to ensure stable estimates of change in both NEPS and PIAAC-L. Using these criteria, we chose to code age in four groups: young adults (18--34 years), prime-age adults (35--44 years), mid-aged adulthood (45--54 years) and older adults (55 years and older). We chose to code educational qualification in three groups according to the International Standard Classification of Education (ISCED 1997): lower or upper secondary ("low"; ISCED levels 0--3), post-secondary or tertiary vocational education ("intermediate"; ISCED levels 4 and 5B), and tertiary academic education ("high"; ISCED levels 5A and 6). For gender, we used a binary variable distinguishing men from women (PIAAC and NEPS did not yet include a diverse/third gender category).

# Results

## Descriptive statistics on skills and skill change

Before reporting our two outcomes of focal interest--mean-level change and rank-order consistency--we first inspected descriptive statistics on skills and skill change in the original raw metric of the tests (0-500 points in PIAAC; logits in NEPS). These are shown in Table\ \@ref(tab:table1) (based on unweighted sample data) and supplementary Table \@ref(tab:tableA1) (based on weighted data). Most important, these descriptive statistics show that the means (*M*) and standard deviations (*SD*) of both literacy and numeracy hardly changed between $T_1$ and $T_2$, although in NEPS the standard deviations of skills at $T_2$ were somewhat smaller than at $T_1$. Furthermore, although the means of the change scores ($\Delta{T_1, T_2)}$ of literacy and numeracy indicated that skills hardly changed over time on average, both the range and the *SD* of the change scores suggested that there were substantial individual differences in change, and that skill gains as well as losses occurred in the samples. This applied to both skills and surveys.

(ref:figure1-caption) Distribution of the change scores ($\Delta{T_1, T_2)}$ in the original metric of the tests (0-500 points in PIAAC-L; logits in NEPS). The dashed vertical line in the center represents "no change" ($\Delta{T_1, T_2} = 0$). The outer two lines represent gains or losses greater than 0.8 standard deviations (using the pooled *SD* across both time points), which is conventionally seen as a "large" effect. The plot is based on a single PV and uses weighted data.

```{r figure1, echo = FALSE, warning = FALSE,  fig.dim = c(8,7), fig.cap = "(ref:figure1-caption)", out.extra = ""}


delta_frequency <- function(input_data) {
  
  plotdata <- get(input_data) %>% 
    filter(.imp == 5) %>% 
    mutate(delta =  t2_pv - t1_pv) %>% 
    svydesign(ids = ~psu, strata = ~stratum, weights = use_weight,
              nest = TRUE, data = .)
  
  
  domain <- if_else(grepl("reading", input_data), "Literacy", "Numeracy")
  study <- if_else(grepl("neps", input_data), "NEPS", "PIAAC-L")
  main <- str_c(domain, " (", study,  ") ")
  
  
  sds  <- deltas %>% 
    filter(target == input_data & grouping == "total") %>% 
    select(sd_pooled, sd_delta)
  
  xmin <- if_else(grepl("neps", input_data), -2.25, -125)
  xmax <- if_else(grepl("neps", input_data), 2.25, 125)
  
  low <-  -0.8 * sds$sd_pooled
  high <- 0.8 *  sds$sd_pooled
 # ymax <- if_else(grepl("neps", input_data), 150, 250)
  

  plotresult <- plotdata %>% 
    svyhist(~delta, design = ., probability = FALSE, 
            breaks = 50,
       xlim = c(xmin, xmax),
       ylim = c(0, 225),
       col = alpha("#BDCDD8", 1),
       border = "#005B96",
       xlab = expression(paste("Change score (", Delta, "T"[1], ", ", "T"[2], ")")),
       main = main)
  abline(v= 0, lty = 2, lwd = 1.5, col = "#CC0001")
  abline(v= low, lty = 2, lwd = 1.5, col = "#CC0001")
  abline(v= high, lty = 2, lwd = 1.5, col = "#CC0001")
 #text(x = 0, y = 0, labels = "-0.8 *SD*", col = "#CC0001")

}


par(mfrow = c(2,2))
delta_frequency("reading_piaac")
delta_frequency("reading_neps") 
delta_frequency("math_piaac")
delta_frequency("math_neps")

```

For a more visual grasp on individual differences in skill change, Figure\ \@ref(fig:figure1) provides the frequency distribution of the change scores $\Delta{T_1, T_2}$ in the original (raw) metric of the tests based on a single PV. These frequency distributions confirm that there was substantial variation in the change scores around their near-zero means for both studies and skills (see also \@ref(tab:tableA1)). Moreover, the change scores were approximately symmetrically distributed around zero, implying that skill gains and losses were similarly prevalent. The distributions were somewhat flatter (i.e., had lower kurtosis) in NEPS than in PIAAC-L, meaning that a relatively larger share experienced stronger change than in PIAAC-L -- as one might expect from the fact that the time between $T_1$ and $T_2$ was twice as long in NEPS than in PIAAC-L.

## Mean-level change in literacy and numeracy skills

(ref:figure2-caption) Mean-level change in literacy and numeracy skills across three years in PIAAC-L and six years in NEPS (based on weighted data).

Figure\ \@ref(fig:figure2) shows results for our first measure of change: mean-level change in literacy (blue) and numeracy (red), expressed in the standardized effect size Cohen's $d_{av}$, across three years (PIAAC-L, left panel) to six (NEPS, right panel) years of adulthood in the total population and in the subgroups. The horizontal lines represent 95% confidence intervals (CIs)

```{r figure2, echo = FALSE, warning = FALSE, fig.cap = "(ref:figure2-caption)", out.extra = ""}

knitr::include_graphics("02_results/deltas.png")

```

In PIAAC-L, there was little evidence for mean-level change across three years in literacy in the total population. The same applied to numeracy. We next looked at change separately in sociodemographic subgroups by comparing different rows of the blue dots (for literacy) or red triangles (for numeracy) within each rectangle (representing socio-demographic subgroups). A few differences between socio-demographic subgroups emerged for educational strata and age groups: For literacy (but not numeracy), there were small gains among the highly educated but no change (or even small losses) among the intermediate and low education groups. Moreover,there were small gains in both literacy and numeracy in the youngest age group (18--34 years) but no change or losses in all older age groups.

In NEPS, too, there was little mean-level change across six years in literacy or numeracy in the total population, apart from a minor tendency toward small gains in literacy and small losses in numeracy. Differences between socio-demographic tendencies were also minor but somewhat more pronounced than in PIAAC-L. Moreover, the patterns for literacy and numeracy were less similar to each other in NEPS than they were in PIAAC-L. Specifically, there was a small gender difference in numeracy change (men experienced losses whereas women experienced gains over time). For literacy, the gender difference in change was smaller and in the opposite directions, favoring men (but note the widely overlapping CIs). As in PIAAC-L, the youngest age group experienced gains in literacy, whereas the older age groups experienced no change. No such age differences were evident for numeracy. For both literacy and numeracy, there was also a clear educational gradient in skill change: Highly-educated individuals experienced losses in literacy and, to a lesser extent, numeracy skills; conversely, lower-educated individuals experienced no change or even gains. The skill loss in the highly-educated group was still small but non-negligible, given the relatively short time period of six years.

In sum, on average, skills changed little over time in both PIAAC-L and NEPS. Some subgroups experienced slightly more pronounced change, yet only one of these subgroup differences was present in both studies (i.e., literacy gains of the youngest but not older age groups).

## Rank-order consistencies

Figure\ \@ref(fig:figure3) shows the rank-order consistencies of literacy and numeracy in PIAAC-L (left panel) and NEPS (right panel). For an even more intuitive visualization, supplementary Figure\ \@ref(fig:figureA1) shows scatterplots of $T_1$ for the first five PVs of each skill and study.

(ref:figure3-caption) Rank-order consistencies of literacy and numeracy skills across three years in PIAAC-L and six years in NEPS (based on weighted data). The horizontal lines represent 95% CIs.

```{r figure3, echo = FALSE, warning = FALSE, fig.cap = "(ref:figure3-caption)", out.extra = ""}

knitr::include_graphics("02_results/cors.png")

```

In PIAAC-L, three-year rank-order consistencies for both literacy and numeracy were both above .80 . Variation across subgroups was very limited: For both literacy and numeracy, all correlations ranged between $`r cors %>% filter(grepl("piaac", target)) %>% summarise(min(rho)) %>% printnum()` \leq r_{T_1, T_2} \leq `r cors %>% filter(grepl("piaac", target)) %>% summarise(max(rho)) %>% printnum()`$. The correlations in different subgroups also had widely overlapping CIs.

In NEPS, a slightly different picture emerged. For literacy, the six-year rank-order consistencies were lower than in PIAAC, and roughly as high as one would expect from the twice as long time interval between assessments (squaring the PIAAC-L rank order consistency gives $r = `r .83^2`$). (The lower rank-order consistency is also evident from the larger scatter of the points and the flatter regression line in Figure\ \@ref(fig:figureA1).) For numeracy, the rank-order consistencies in the total sample and all but one subgroups were higher ($`r cors %>% filter(target == "math_neps") %>% summarise(min(rho)) %>% printnum()` \leq r_{T_1, T_2} \leq `r cors %>% filter(target == "math_neps") %>% summarise(max(rho)) %>% printnum()`$) than those for literacy ($`r cors %>% filter(target == "reading_neps") %>% summarise(min(rho)) %>% printnum()` \leq r_{T_1, T_2} \leq `r cors %>% filter(target == "reading_neps") %>% summarise(max(rho)) %>% printnum()`$), and only minimally smaller than those in PIAAC-L. Within each skill, there was little variation across sociodemographic subgroups as in PIAAC, with confidence intervals overlapping widely.The only discernible tendency was that for slightly lower rank-order consistencies in older age groups, which might simply arise from the slightly lower standard deviation in this group and would be likely to shrink when correcting for restriction on range.

In sum, the rank-order consistencies -- indicative of "differential stability" of skills -- further reinforced the impression from Figure\ \@ref(fig:figure1) that a great deal of individual differences in change lie behind the small-to-zero mean-level changes.

## Additional Analyses: Cross-Sectional Age Profiles and Linear Age Differences

In additional analyses, we examined the cross-sectional age profiles of literacy and numeracy skills. Although not of primary interest to our present study, these cross-sectional age profiles provide an interesting point of reference against which to compare our longitudinal findings.

First, we plotted the cross-sectional age profiles of literacy and numeracy skills (based on a single PV and using survey weights). These plots are shown in supplementary Figure\ \@ref(fig:figureA2). They confirm that literacy and numeracy in PIAAC-L and NEPS exhibited basically the same age profiles know from earlier studies based on cross-sectional data [e.g., @desjardins2012; @paccagnella2016; @salthouse2019; @oecd2016]: Specifically, skills tended to remain largely stable until an inflection point around age 40 to 50, beyond which they declined slightly. Cross-sectional age differences (i.e., age-related declines) appeared slightly more pronounced in NEPS than in PIAAC-L.

Second, we estimated linear age differences in skills per year, per study period (3 or 6 years), and life decade implied by the cross-sectional $T_1$ data. The results are shown in supplementary Table\ \@ref(tab:tableA2). These estimates served as a point of comparison for our longitudinal estimates of mean-level change over 3 years (PIAAC-L) to 6 years (NEPS). In PIAAC-L, the cross-sectional age differences were close to the longitudinal estimates of mean-level change in the total sample shown in Figure\ \@ref(fig:figure2) and Tables\ \@ref(tab:table1) and \@ref(tab:tableA1). In NEPS, by contrast, the cross-sectional age differences mostly exceeded the longitudinal estimates of mean-level change we found and suggested losses that were not present in the longitudinal data. For example, numeracy would seem to drop by about $-1 \ SD$ between age 40 and age 60 according to the cross-sectional age differences in $T_1$ numeracy; the mean difference in numeracy skills between the youngest (24--34 years) and oldest (55+ years) age groups according to the cross-sectional data amounted to $`r age_diffs %>% filter(target == "math_neps") %>% pluck("young_old_sd") %>% printnum()` \ SD$. As shown in Table\ \@ref(tab:tableA2), cross-sectional age difference of numeracy in NEPS suggested that numeracy delines at an average rate of $`r age_effects %>% filter(Change == "Per decade (SD)") %>% select("Numeracy (NEPS)")` \ SD$ per decade of life (or `r age_effects %>% filter(Change == "Per period (SD)") %>% select("Numeracy (NEPS)")` *SD* for the six-year study period). These numbers differ from the longitudinal estimates of mean-level change and are generally larger. The situation was similar for literacy in NEPS. In sum, these analyes suggest that the cross-sectional age profiles are not necessarily an accurate estimate of mean-level changes estimated from repeated-measures data.

# Discussion

Are literacy and numeracy skills impervious to change after schooling age -- or do they continue to change throughout adulthood? Does change involve gains or losses, and how are gains and losses distributed in the different segments of the adult population? To answer these questions, we analyzed data from two large-scale assessment surveys offering repeated measures of adults' literacy and numeracy skills spaced three (PIAAC-L) to six (NEPS) years apart. We considered two indicators of skill change: absolute change (based on the difference score between two time points) and relative change (based on the correlation between skills at two time points).

Our analyses yielded three main insights. First, on average, there was little absolute change in literacy or numeracy over the three-to-six-year periods in PIAAC-L and NEPS. Although there was a tendency toward small gains in literacy (but not numeracy) in both studies, effect sizes for mean-level change hovered around zero for both skills and in both studies ($`r deltas %>% filter(grepl("total", grouping)) %>% select(dav) %>% min() %>% printnum()` \leq d \leq `r deltas %>% filter(grepl("total", grouping)) %>% select(dav) %>% max() %>% printnum()`$). Some segments of the population experienced slightly more pronounced mean-level change. For example, there was a tendency for the youngest age group to experience literacy (and in PIAAC-L also numeracy) gains, whereas all other age groups experienced no change or even small losses. However, most socio-demographic differences in skill change were small, and few of them were identical across studies. In fact, one of the differences was in opposite direction in the two studies: In PIAAC-L, the highly-educated experienced gains in literacy over time, whereas lower-educated groups segments of the population not. By contrast, In NEPS, the most highly educated group experienced losses in literacy and numeracy skills, while the lower-educated groups experienced gains (or no change). The losses in literacy skills among the highly educated in NEPS was the strongest mean-level difference we observed ($d = `r deltas %>% select(dav) %>% min()`)$; change of about a fourth of a standard deviation is sizable, given the relatively short period of six years. The stronger declines among the highly educated observed in NEPS are in line with the cross-sectional age profiles of different educational groups reported by @paccagnella2016 based on PIAAC international, which he labeled as "surprising" (p. 66). The literacy gains among the highly educated in PIAAC-L are at odds with Paccagnella's cross-sectional findings and suggest a "Matthew effect" [@dannefer1987; @blossfeld2011a]. It is unclear what what causes these differences in findings between the two surveys, and it will be important for future research to pinpoint methodological characteristics of surveys that can explain these differences (see Limitations and Directions for Future Research). A potential (though currently speculative) explanation is that the differences in findings reflect floor effects (for the highly-educated in PIAAC-L and the lower-educated in NEPS) and/or ceiling effects (for the higher-educated in NEPS effects) in the assessments. Because more highly-educated respondents are also more highly skilled, the educational gradient in literacy and numeracy change in NEPS might also reflect stronger regression toward the mean in NEPS compared to PIAAC-L. Regression toward the mean is a ubiquitous phenomenon in may human characteristics and implies that respondents with extremely high or low scores are likely to score less extremely in a retest. Supporting this conjecture, additional analyses showed that the $T_1$ literacy and numeracy scores correlated with the change score at $`r cors_t1_delta %>% filter(study == "neps") %>% select(rho) %>% min()` \leq r \leq `r cors_t1_delta %>% filter(study == "neps") %>% select(rho) %>% max() %>% printnum()`$ but only at $`r cors_t1_delta %>% filter(study == "piaac") %>% select(rho) %>% min()` \leq r \leq `r cors_t1_delta %>% filter(study == "piaac") %>% select(rho) %>% max() %>% printnum()`$ in PIAAC-L. Also in line with this explanation, additional analyses shown in Table\ \@ref(tab:tableA3) demonstrated that mean-level change in skills was strongly dependent on initial skill level (quartile) in NEPS but less so in PIAAC-L. Although in both studies the quartile with the highest skills at \$T_1\$ experienced losses and the quartile with the lowest skills experienced gains (indicating regression to the mean), the spread between the first and fourth quartile was much more pronounced in NEPS than in PIAAC-L. Moreover, recent studies investigating literacy skill change in NEPS separately by initial skill level [@wicht2021] showed that very low or very high $T_1$ literacy values regressed to the mean at $T_2$, and that higher-educated individuals in NEPS experience relative gains, not losses, in literacy, after controlling for initial skill levels in regression [@wicht2020]. Thus, it is possible that assessment characteristics may explain the differences in the educational gradients of skill change between the two survey, but further research is needed to test this explanation. At any rate, the differences highlight the importance of carefully considering assessment design and respondent characteristics when interpreting results from large-scale assessment surveys [for a related argument, see @pohl2021].

Second, behind the mostly small mean-level changes in the total population and socio-demographic segments, we found a considerable degree of individual differences in skill change. A closer look at the distribution of the change scores $\Delta{T_1, T_2}$ confirmed that there was substantial variation around the near-zero means per skill and study (see \@ref(tab:tableA1) and Figure\ \@ref(fig:figure1). The change scores were approximately symmetrically distributed around their near-zero mean. This implies that gains and losses in literacy and numeracy over time were nearly equally prevalent. The rank-order consistencies in the total population $r_{T_1, T_2}$ further reinforced the view that a substantial share of respondents experience skill change that is not evident in the mean-level changes: These rank-order consistencies were high but not perfect across the three-year period in PIAAC ($`r cors %>% filter(grepl("piaac", target) & grepl("total", grouping)) %>% select(rho) %>% min() %>% printnum() ` \leq r_{T_1, T_2} \leq `r cors %>% filter(grepl("piaac", target) & grepl("total", grouping)) %>% select(rho) %>% max() %>% printnum()`$) and only moderate across the six-year period in NEPS ($`r cors %>% filter(grepl("neps", target) & grepl("total", grouping)) %>% select(rho) %>% min() %>% printnum() ` \leq r_{T_1, T_2} \leq `r cors %>% filter(grepl("neps", target) & grepl("total", grouping)) %>% select(rho) %>% max() %>% printnum()`$). This implies that individual differences in skills were not fully stable over time because some individuals experienced skill change that changes the ranking of individuals in the skill distribution. Literacy was significantly less rank-order consistent than numeracy in NEPS, yet this difference did not replicate in PIAAC where, if anything, literacy appeared slightly more consistent. Subgroup differences in rank-order consistencies were no more pronounced than those in mean-level change (and those that emerged might partly be explained by restriction on range when splitting the population in subsegments). The only tendency that emerged was that literacy (and to a lesser extent numeracy) were less rank-order stable in older compared to younger age groups in NEPS (but not PIAAC). Because of the limited sample size in these subgroups, the statistical uncertainty about these difference is high, however. Thus, we conclude that sociodemographic subgroups do not differ strongly in change and stability of literacy and numeracy skills. Together, our results suggest that the change in skills that can be observed is mostly idiosyncratic (i.e., specific to individuals and not explained by systematic socio-demographic gradients), and individual differences in change dominate the picture.

To put the rank-order consistencies of literacy and numeracy into perspective, several studies on cognitive development and aging found higher rank-order consistencies for measures of intelligence over longer periods of time. For example, @gow2012, in one of the few longitudinal studies on the stability of cognitive ability across the life span, found that general intelligence (*g*) measured with the Moray House Test No. 12 (MHT) at age 11 correlated at $r = .67$ ($r = .78$ after disattenuation) with intelligence at age 70 in 1,017 Scottish individuals from the Lothian Birth Cohort study. Other cohort studies from Scotland yielded comparable estimates but found rank-order consistencies to decline somewhat as the age of individuals at retest further increased into the eigth decade of life [@deary2014]. Similarly, @schalke2013 reported rank-order consistencies of $r = .85$ for *g* from age 12 to age 52, with only slightly lower estimates for more specific abilities (e.g., fluid reasoning, comprehension knowledge, visual processing) among 344 individuals from Luxembourg. One should bear in mind that the measures of intelligence in these studies are not directly comparable to the measures of literacy and numeracy in NEPS [recall that literacy and numeracy are best viewed as broad abilities on Stratum II in the updated CHC model of intelligence; @mcgrew2009]. Still, it is clear that the rank-order consistencies of literacy and numeracy over three to six years in PIAAC-L and NEPS are relatively low in comparison to that of general intelligence in these studies.

Finally, because most prior research on age-related differences in literacy and numeracy was based on cross-sectional data [e.g., @paccagnella2016; @desjardins2012], we also compared the age differences suggested by the cross-sectional age profiles to the longitudinal estimates of mean-level change. The age differences implied by the cross-sectional data were roughly similar to our longitudinal results on mean-level change in PIAAC-L but differed markedly from them in NEPS. These divergences highlight the importance of using repeated-measures designs to properly gauge age effects on skills. At the same time, there are some indications from prior research that repeated participation in cognitive assessments (retest effects) may bias stability estimates -- and that, for this reason, cross-sectional age profiles can sometimes capture age effects on cognitive skills more accurately [@salthouse2019]. The extent to which this applies in modern cognitive assessments in which only a subset of items are repeatedly administered (i.e., the "anchor items") is unclear. Further research is needed to pinpoint the reasons behind the divergences of the cross-sectional age profiles and longitudinal mean-level changes in NEPS.

## Limitations and Directions for Future Research

The major advancement made by our study was to approach the question of change and stability in adults' literacy and numeracy skills through the lens of repeated-measures data *and* large-scale, non-selective samples, and to apply PV methodology [@wu2005; @vondavier2009] to account for measurement error in skills. This combination has so far been rare, if not absent from the literature. Previous evidence on age-related changes in literacy and numeracy skills hails either from large-scale cross-sectional or small-scale (and often highly selective) longitudinal studies [@paccagnella2016; @desjardins2012; @reder2009]. We are not aware of any comparably rich longitudinal data sources on adults' literacy and numeracy skills. Still, our study leaves several questions unanswered that future research should address.

First, PIAAC-L and NEPS covered only two measurement occasions spanning time periods of three to six years. As our analyses show, even such relatively limited time periods allow for change in literacy and numeracy to occur. Nonetheless, six years represent less than a tenth of the current average life span of about 80 years in Germany and similar Western nations. Hence, future studies that follow individuals over longer time periods and assess literacy and numeracy at multiple time points would enable more nuanced insights into the temporal dynamics of change in literacy and numeracy (e.g., though growth curve modeling). Of note, it is not necessarily the case that rank-order consistencies are lower for longer time intervals, and it will be interesting how the stability of literacy and numeracy plays out in the long-run. So far, however, no truly large-scale, long-running panel studies measuring adults' literacy and numeracy skills in the sense of broad competencies [^6] at multiple time points are available, even though NEPS is ongoing and will continue to grow in this regard.

[^6]: Note that there is handful of longitudinal studies on more narrow, circumscribed aspects of intelligence from research on aging [for discussions, see @deary2014]. However, these studies rarely measure broad competence domains such as literacy and numeracy, and some of them do not conform with modern psychometric standards.

Second, as noted, the assessments in PIAAC and NEPS were not high-stakes assessments. Although interviewers in both studies did their best to ensure that respondents took the tests seriously, the tests are more likely to represent "typical" as opposed to "maximum" performance. Whether typical or maximum performance in more relevant for life outcomes can be debated, yet is clear that high-stakes assessments might lead to different estimates of stability and change. In high-stakes assessment, performance tends to depend less on extraneous factors such as test motivation because most individuals are highly motivated in high-stakes assessments. Because it is almost impossible to implement repeated-measures high-stakes assessments, future studies should investigate whether extraneous factors such as test motivation influence estimates of change and stability in literacy an numeracy.

Third, some results differed between PIAAC-L and NEPS. This pertains, for example, to the relative rank-order consistency of literacy and numeracy and to the differences in mean-level change across educational strata. That findings partly differ across studies may not be surprising, as PIAAC-L and NEPS are independent studies that use similar but not identical assessment approaches. However, in order to build a robust body of evidence on skill change in adulthood, it will be crucial for future research to understand which specific characteristics of the research and assessment design in such surveys may influence findings regarding stability and change in adults' skills. Possible candidates of such characteristics include (but are not limited to) sampling approach and sample composition, test-retest interval, selective non-response and panel dropout, assessment design (e.g., speeded vs. non-speeded; computer-based vs. paper-pencil), PV-generating models, and others. As described under Method, PIAAC-L and NEPS differ with regard to all of these characteristics [for a discussion, see @durda2020]. For example, the literacy/reading competence and numeracy/mathematical competence tests of both surveys are highly but not perfectly correlated [@carstensen2017]. One can speculate that the NEPS assessment -- which is designed to enable comparisons with the other NEPS cohorts, including children and youth [@weinert2011; @gehrer2013] -- is more susceptible to bottom and/or ceiling effects and regression to the mean than that of PIAAC, which is tailored to adults. Both studies suffer from selective nonresponse panel dropout, especially of those with lower education and skills [@martin2020]. Although we used longitudinal weights to remedy the potential bias, weights may not fully eliminate bias, especially if there is selectivity with regard to unobserved respondent characteristics.

## Conclusion

Our findings suggest that literacy and numeracy skills are not "set like plaster" in adulthood and, despite their partial heritability [for a meta-analysis on literacy, see @andreola2020], show lifelong plasticity. Although literacy and numeracy changed little on average, there are considerable individual differences in change (i.e., the variation in the change scores). Moreover, the rank-order consistencies of skills are far from unity. This suggests that many individuals do experience change in literacy and numeracy even over relatively short time periods of three to six years. This change comprises gains and losses in almost equal share, such that gains and losses cancel each other out when looking only at mean-level change in the population. Few systematic differences in change across major sociodemographic segments emerged, and these differences mostly did not replicate across studies. The opposing pattern for mean-level change by educational attainment was the most striking example and highlights the importance of considering characteristics such as sample, assessment design, test--retest interval, and statistical models in future research, as findings regarding (subgroup differences in) literacy and numeracy development may partly depend on such study characteristics.

Overall, then, it appears that the change in skills during adulthood that can be observed is mostly idiosyncratic: Individual differences in change (not population-level gains or losses, or systematic differences in skill change across sociodemographic subgroups) dominate the picture. The upside of this findings is that skill loss is not inevitable and that gains are still possible. This directs attention to the factors that govern individual's proficiency gains over time. Examples of factors underlying skill change that recent studies using longitudinal data (including PIAAC-L and NEPS) have identified include skill use or practice engagement [e.g., @reder2020; @reder2009], continued education and training [e.g., @gauly2020; @gauly2019], labor market participation [@wicht2020; @wicht2021], or basic skill programmes [@reder2009; @wolf2014]. Some of these factors may provide apt targets for policy and interventions. Future research should, therefore, aim to pinpoint further factors (e.g., health) that explain individual differences and ideally using longitudinal data and causal designs.

\newpage

# Data availability statement

The data from the two studies used in this investigation are public and available upon registration from the NEPS and PIAAC/PIAAC-L websites, respectively.

Data from the National Educational Panel Study (NEPS): Starting Cohort Adults (<doi:10.5157/NEPS:SC6:11.0.0>) are available from <https://www.neps-data.de/Data-Center/Data-and-Documentation/Start-Cohort-Adults> Data from PIAAC-L are available from <https://www.gesis.org/en/piaac/rdc/data/piaac-longitudinal>.

For full transparency, the complete R code for the analyses and the R markdown file for the manuscript [created with the R package papaja; @aust2020] are available from the first author's github repository at [blinded for review].

\newpage

# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

\endgroup

